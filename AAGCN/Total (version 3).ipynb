{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ibmelab/Documents/GG/VSLRecognition/AUTSL/AAGCN/\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "\n",
    "#folder_path = r'dataset'  # Use raw string to avoid issues with backslashes\n",
    "#folder_path = r'30 class 28 actor (center)'\n",
    "folder_path = r'/home/ibmelab/Documents/GG/VSLRecognition/AUTSL/videos'\n",
    "csv_file_path = 'videos_list.csv'\n",
    "labels_file_path = '1_1000_label.csv'\n",
    "final_file_path = 'temp_videos_list.csv'\n",
    "\n",
    "# Read the label and gloss mapping\n",
    "label_to_gloss = {}\n",
    "with open(labels_file_path, mode='r', encoding='utf-8') as labels_file:\n",
    "    csv_reader = csv.DictReader(labels_file)\n",
    "    for row in csv_reader:\n",
    "        label = int(row['id_label_in_documents'])\n",
    "        gloss = row['name']\n",
    "        label_to_gloss[label] = gloss\n",
    "\n",
    "# Write video names, labels, and glosses to a CSV file\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['file', 'label', 'gloss'])\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.mp4', '.mkv', '.avi', '.mov', '.flv', '.wmv')):\n",
    "            match = re.search(r'_(\\d+)\\.', filename)\n",
    "            if match:\n",
    "                label = int(match.group(1))\n",
    "                gloss = label_to_gloss.get(label, 'Unknown')\n",
    "            else:\n",
    "                label = 'N/A'\n",
    "                gloss = 'Unknown'\n",
    "\n",
    "            full_filename = os.path.join(folder_path, filename)\n",
    "            csv_writer.writerow([full_filename, label, gloss])\n",
    "\n",
    "print(f'Video names have been written to {csv_file_path}')\n",
    "\n",
    "# Find min label\n",
    "with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    labels = [int(row[\"label\"]) for row in csv_reader if row[\"label\"].isdigit()]  # Convert to int and filter out 'N/A'\n",
    "    min_label = min(labels) if labels else None\n",
    "\n",
    "print(\"Minimum label:\", min_label)\n",
    "\n",
    "# Normalize labels\n",
    "with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csv_file, \\\n",
    "     open(final_file_path, mode='w', newline='', encoding='utf-8') as final_file:\n",
    "    \n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    fieldnames = csv_reader.fieldnames\n",
    "    \n",
    "    csv_writer = csv.DictWriter(final_file, fieldnames=fieldnames)\n",
    "    csv_writer.writeheader()\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        if row['label'].isdigit():  # Check if label is a digit before converting\n",
    "            row['label'] = str(int(row['label']) - min_label)  # Normalize and convert back to string\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "# Replace the original file with the updated file\n",
    "os.replace(final_file_path, csv_file_path)\n",
    "\n",
    "print(\"Labels have been updated and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ibmelab/Documents/GG/VSLRecognition/vsl/AAGCN/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "\n",
    "videos_df = pd.read_csv('/home/ibmelab/Documents/GG/VSLRecognition/vsl/label1-200/full_data_1_200.csv')\n",
    "videos_df['label'] = videos_df['file_name'].apply(lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "def extract_actor_name(file_name):\n",
    "    parts = file_name.split('_')\n",
    "    if len(parts) > 1:\n",
    "        return '_'.join(parts[:2])\n",
    "    return file_name.strip()\n",
    "\n",
    "videos_df['actor'] = videos_df['file_name'].apply(lambda x: extract_actor_name(x.split('\\\\')[-1]))\n",
    "\n",
    "#print(videos_df['actor'])\n",
    "\n",
    "unique_actors = videos_df['actor'].unique()\n",
    "train_actors = random.sample(list(unique_actors), 24)\n",
    "remaining_actors = [actor for actor in unique_actors if actor not in train_actors]\n",
    "valid_actors = random.sample(remaining_actors, 2)\n",
    "test_actors = [actor for actor in remaining_actors if actor not in valid_actors]\n",
    "\n",
    "train_videos = videos_df[videos_df['actor'].isin(train_actors)][['file_name', 'label']]\n",
    "valid_videos = videos_df[videos_df['actor'].isin(valid_actors)][['file_name', 'label']]\n",
    "test_videos = videos_df[videos_df['actor'].isin(test_actors)][['file_name', 'label']]\n",
    "\n",
    "train_videos.to_csv('train.csv', index=False)\n",
    "valid_videos.to_csv('valid.csv', index=False)\n",
    "test_videos.to_csv('test.csv', index=False)\n",
    "\n",
    "unique_actors.sort()\n",
    "#print(unique_actors)\n",
    "\n",
    "print(f'Train videos: {len(train_videos)}')\n",
    "print(f'Validation videos: {len(valid_videos)}')\n",
    "print(f'Test videos: {len(test_videos)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hand_landmarks = ['INDEX_FINGER_DIP', 'INDEX_FINGER_MCP', 'INDEX_FINGER_PIP', 'INDEX_FINGER_TIP', \n",
    "                  'MIDDLE_FINGER_DIP', 'MIDDLE_FINGER_MCP', 'MIDDLE_FINGER_PIP', 'MIDDLE_FINGER_TIP', \n",
    "                  'PINKY_DIP', 'PINKY_MCP', 'PINKY_PIP', 'PINKY_TIP', 'RING_FINGER_DIP', 'RING_FINGER_MCP', \n",
    "                  'RING_FINGER_PIP', 'RING_FINGER_TIP', 'THUMB_CMC', 'THUMB_IP', 'THUMB_MCP', 'THUMB_TIP', 'WRIST']\n",
    "pose_landmarks = ['LEFT_ANKLE', 'LEFT_EAR', 'LEFT_ELBOW', 'LEFT_EYE', 'LEFT_EYE_INNER', 'LEFT_EYE_OUTER', \n",
    "                  'LEFT_FOOT_INDEX', 'LEFT_HEEL', 'LEFT_HIP', 'LEFT_INDEX', 'LEFT_KNEE', 'LEFT_PINKY', \n",
    "                  'LEFT_SHOULDER', 'LEFT_THUMB', 'LEFT_WRIST', 'MOUTH_LEFT', 'MOUTH_RIGHT', 'NOSE', \n",
    "                  'RIGHT_ANKLE', 'RIGHT_EAR', 'RIGHT_ELBOW', 'RIGHT_EYE', 'RIGHT_EYE_INNER', 'RIGHT_EYE_OUTER', \n",
    "                  'RIGHT_FOOT_INDEX', 'RIGHT_HEEL', 'RIGHT_HIP', 'RIGHT_INDEX', 'RIGHT_KNEE', 'RIGHT_PINKY', \n",
    "                  'RIGHT_SHOULDER', 'RIGHT_THUMB', 'RIGHT_WRIST']\n",
    "\n",
    "def extract_keypoint(video_path, label):\n",
    "    video =   f\"/home/ibmelab/Documents/GG/VSLRecognition/vsl/videos/{video_path}\"\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    \n",
    "    keypoint_dict = defaultdict(list)\n",
    "    count = 0\n",
    "\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            count += 1\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = holistic.process(image)\n",
    "\n",
    "            if results.right_hand_landmarks:\n",
    "                for idx, landmark in enumerate(results.right_hand_landmarks.landmark): \n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_right_x\"].append(landmark.x)\n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_right_y\"].append(landmark.y)\n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_right_z\"].append(landmark.z)\n",
    "            else:\n",
    "                for idx in range(len(hand_landmarks)):\n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_right_x\"].append(0)\n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_right_y\"].append(0)\n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_right_z\"].append(0)\n",
    "\n",
    "            if results.left_hand_landmarks:\n",
    "                for idx, landmark in enumerate(results.left_hand_landmarks.landmark): \n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_left_x\"].append(landmark.x)\n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_left_y\"].append(landmark.y)\n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_left_z\"].append(landmark.z)\n",
    "            else:\n",
    "                for idx in range(len(hand_landmarks)):\n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_left_x\"].append(0)\n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_left_y\"].append(0)\n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_left_z\"].append(0)\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "                for idx, landmark in enumerate(results.pose_landmarks.landmark): \n",
    "                    keypoint_dict[f\"{pose_landmarks[idx]}_x\"].append(landmark.x)\n",
    "                    keypoint_dict[f\"{pose_landmarks[idx]}_y\"].append(landmark.y)\n",
    "                    keypoint_dict[f\"{pose_landmarks[idx]}_z\"].append(landmark.z)\n",
    "            else:\n",
    "                for idx in range(len(pose_landmarks)):\n",
    "                    keypoint_dict[f\"{pose_landmarks[idx]}_x\"].append(0)\n",
    "                    keypoint_dict[f\"{pose_landmarks[idx]}_y\"].append(0)\n",
    "                    keypoint_dict[f\"{pose_landmarks[idx]}_z\"].append(0)\n",
    "\n",
    "        keypoint_dict[\"frame\"] = count\n",
    "        keypoint_dict[\"video_path\"] = video_path\n",
    "        keypoint_dict[\"label\"] = label\n",
    "\n",
    "        return keypoint_dict\n",
    "\n",
    "def process_videos(mode):\n",
    "    csv_file = f\"/home/ibmelab/Documents/GG/VSLRecognition/vsl/label1-200/label/labelRight/{mode}_labels.csv\"\n",
    "    data = pd.read_csv(csv_file)\n",
    "\n",
    "    keypoints_list = Parallel(n_jobs=-1)(  # Chạy song song với số lượng core tối đa\n",
    "        delayed(extract_keypoint)(row['file_name'], row['label_id']) for index, row in data.iterrows()\n",
    "    )\n",
    "\n",
    "    # Tạo DataFrame và lưu vào CSV\n",
    "    keypoints_df = pd.DataFrame(keypoints_list)\n",
    "    keypoints_df.to_csv(f\"{mode}_set.csv\", index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    modes = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "    for mode in modes:\n",
    "        process_videos(mode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try something new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ibmelab/Documents/GG/VSLRecognition/AUTSL\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_index(array):\n",
    "    for i, num in enumerate(array):\n",
    "        if num != 0:\n",
    "            return i\n",
    "    return -1  # Return -1 if no non-zero element is found\n",
    "\n",
    "def curl_skeleton(array):\n",
    "    if sum(array) == 0:\n",
    "        return array\n",
    "    for i, location in enumerate(array):\n",
    "        if location != 0:\n",
    "            continue\n",
    "        else:\n",
    "            if i == 0 or i == len(array) - 1:\n",
    "                continue\n",
    "            else:\n",
    "                if array[i + 1] != 0:\n",
    "                    array[i] = float((array[i - 1] + array[i + 1]) / 2)\n",
    "                else:\n",
    "                    j = find_index(array[i + 1:])\n",
    "                    if j == -1:\n",
    "                        continue\n",
    "                    array[i] = float(((1 + j) * array[i - 1] + array[i + 1 + j]) / (2 + j))\n",
    "    return array\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hand_landmarks = [\n",
    "        'INDEX_FINGER_DIP', 'INDEX_FINGER_MCP', 'INDEX_FINGER_PIP', 'INDEX_FINGER_TIP',\n",
    "        'MIDDLE_FINGER_DIP', 'MIDDLE_FINGER_MCP', 'MIDDLE_FINGER_PIP', 'MIDDLE_FINGER_TIP',\n",
    "        'PINKY_DIP', 'PINKY_MCP', 'PINKY_PIP', 'PINKY_TIP',\n",
    "        'RING_FINGER_DIP', 'RING_FINGER_MCP', 'RING_FINGER_PIP', 'RING_FINGER_TIP',\n",
    "        'THUMB_CMC', 'THUMB_IP', 'THUMB_MCP', 'THUMB_TIP', 'WRIST'\n",
    "    ]\n",
    "    \n",
    "    HAND_IDENTIFIERS = [id + \"_right\" for id in hand_landmarks] + [id + \"_left\" for id in hand_landmarks]\n",
    "    POSE_IDENTIFIERS = [\"RIGHT_SHOULDER\", \"LEFT_SHOULDER\", \"LEFT_ELBOW\", \"RIGHT_ELBOW\"]\n",
    "    body_identifiers = HAND_IDENTIFIERS + POSE_IDENTIFIERS\n",
    "\n",
    "    modes = [\"train\", \"valid\", \"test\"]\n",
    "    output_folder = \"hand_keypoints\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for mode in modes:\n",
    "        print(f\"Processing {mode}.csv\")\n",
    "        dataset = pd.read_csv(f\"AAGCN/{mode}_set.csv\")\n",
    "        print(f\"Number of videos in {mode} set: {len(dataset)}\")\n",
    "        \n",
    "        for video_index, video in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "            video_name = video[\"video_path\"]  # Assuming there's a 'video_path' column\n",
    "            video_base = os.path.splitext(os.path.basename(video_name))[0]  # Get the base name of the video\n",
    "\n",
    "            T = len(ast.literal_eval(video[\"INDEX_FINGER_DIP_right_x\"]))\n",
    "            num_keypoints = len(body_identifiers)\n",
    "            keypoints_all_frames = np.empty((T, num_keypoints, 2))\n",
    "            \n",
    "            for index, identifier in enumerate(body_identifiers):\n",
    "                data_keypoint_preprocess_x = curl_skeleton(ast.literal_eval(video[identifier + \"_x\"]))\n",
    "                data_keypoint_preprocess_y = curl_skeleton(ast.literal_eval(video[identifier + \"_y\"]))\n",
    "                keypoints_all_frames[:, index, 0] = np.asarray(data_keypoint_preprocess_x)\n",
    "                keypoints_all_frames[:, index, 1] = np.asarray(data_keypoint_preprocess_y)\n",
    "            \n",
    "            # Tạo thư mục đầu ra cho video\n",
    "            video_output_folder = os.path.join(output_folder, video_base)\n",
    "            os.makedirs(video_output_folder, exist_ok=True)\n",
    "            \n",
    "            # Lưu dữ liệu keypoint cho từng frame\n",
    "            for idx in range(T):\n",
    "                frame_data = keypoints_all_frames[idx]\n",
    "                output_file = os.path.join(video_output_folder, f\"hand_kp_{idx:05d}.npy\")\n",
    "                np.save(output_file, frame_data)\n",
    "                \n",
    "        print(f\"Processing of {mode} set completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define hand and pose landmarks as per your specification\n",
    "hand_landmarks = [\n",
    "    'INDEX_FINGER_DIP', 'INDEX_FINGER_MCP', 'INDEX_FINGER_PIP', 'INDEX_FINGER_TIP',\n",
    "    'MIDDLE_FINGER_DIP', 'MIDDLE_FINGER_MCP', 'MIDDLE_FINGER_PIP', 'MIDDLE_FINGER_TIP',\n",
    "    'PINKY_DIP', 'PINKY_MCP', 'PINKY_PIP', 'PINKY_TIP',\n",
    "    'RING_FINGER_DIP', 'RING_FINGER_MCP', 'RING_FINGER_PIP', 'RING_FINGER_TIP',\n",
    "    'THUMB_CMC', 'THUMB_IP', 'THUMB_MCP', 'THUMB_TIP', 'WRIST'\n",
    "]\n",
    "\n",
    "HAND_IDENTIFIERS = [id + \"_right\" for id in hand_landmarks] + [id + \"_left\" for id in hand_landmarks]\n",
    "POSE_IDENTIFIERS = [\"RIGHT_SHOULDER\", \"LEFT_SHOULDER\", \"LEFT_ELBOW\", \"RIGHT_ELBOW\"]\n",
    "body_identifiers = HAND_IDENTIFIERS + POSE_IDENTIFIERS  # Total of 46 keypoints\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Function to find the index of the first non-zero element\n",
    "def find_index(array):\n",
    "    for i, num in enumerate(array):\n",
    "        if num != 0:\n",
    "            return i\n",
    "    return -1  # Return -1 if no non-zero element is found\n",
    "\n",
    "# Function to fill in missing keypoints\n",
    "def curl_skeleton(array):\n",
    "    array = list(array)\n",
    "    if sum(array) == 0:\n",
    "        return array\n",
    "    for i, location in enumerate(array):\n",
    "        if location != 0:\n",
    "            continue\n",
    "        else:\n",
    "            if i == 0 or i == len(array) - 1:\n",
    "                continue\n",
    "            else:\n",
    "                if array[i + 1] != 0:\n",
    "                    array[i] = float((array[i - 1] + array[i + 1]) / 2)\n",
    "                else:\n",
    "                    j = find_index(array[i + 1:])\n",
    "                    if j == -1:\n",
    "                        continue\n",
    "                    array[i] = float(((1 + j) * array[i - 1] + array[i + 1 + j]) / (2 + j))\n",
    "    return array\n",
    "\n",
    "def process_video(video_path, save_dir):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    mp_holistic_instance = mp_holistic.Holistic(\n",
    "        min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "    # Prepare a dictionary to store keypoints\n",
    "    keypoint_data = defaultdict(list)\n",
    "    frame_count = 0\n",
    "\n",
    "    with mp_holistic_instance as holistic:\n",
    "        while frame_count < 30:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = holistic.process(image)\n",
    "\n",
    "            # Process right hand\n",
    "            if results.right_hand_landmarks:\n",
    "                for idx, landmark in enumerate(results.right_hand_landmarks.landmark):\n",
    "                    keypoint_data[f\"{hand_landmarks[idx]}_right_x\"].append(landmark.x)\n",
    "                    keypoint_data[f\"{hand_landmarks[idx]}_right_y\"].append(landmark.y)\n",
    "            else:\n",
    "                for idx in range(len(hand_landmarks)):\n",
    "                    keypoint_data[f\"{hand_landmarks[idx]}_right_x\"].append(0)\n",
    "                    keypoint_data[f\"{hand_landmarks[idx]}_right_y\"].append(0)\n",
    "\n",
    "            # Process left hand\n",
    "            if results.left_hand_landmarks:\n",
    "                for idx, landmark in enumerate(results.left_hand_landmarks.landmark):\n",
    "                    keypoint_data[f\"{hand_landmarks[idx]}_left_x\"].append(landmark.x)\n",
    "                    keypoint_data[f\"{hand_landmarks[idx]}_left_y\"].append(landmark.y)\n",
    "            else:\n",
    "                for idx in range(len(hand_landmarks)):\n",
    "                    keypoint_data[f\"{hand_landmarks[idx]}_left_x\"].append(0)\n",
    "                    keypoint_data[f\"{hand_landmarks[idx]}_left_y\"].append(0)\n",
    "\n",
    "            # Process pose landmarks (shoulders and elbows)\n",
    "            if results.pose_landmarks:\n",
    "                landmark_dict = {mp_holistic.PoseLandmark(idx).name: idx for idx in range(len(mp_holistic.PoseLandmark))}\n",
    "                for pose_identifier in POSE_IDENTIFIERS:\n",
    "                    idx = landmark_dict.get(pose_identifier, None)\n",
    "                    if idx is not None:\n",
    "                        landmark = results.pose_landmarks.landmark[idx]\n",
    "                        keypoint_data[f\"{pose_identifier}_x\"].append(landmark.x)\n",
    "                        keypoint_data[f\"{pose_identifier}_y\"].append(landmark.y)\n",
    "                    else:\n",
    "                        keypoint_data[f\"{pose_identifier}_x\"].append(0)\n",
    "                        keypoint_data[f\"{pose_identifier}_y\"].append(0)\n",
    "            else:\n",
    "                for pose_identifier in POSE_IDENTIFIERS:\n",
    "                    keypoint_data[f\"{pose_identifier}_x\"].append(0)\n",
    "                    keypoint_data[f\"{pose_identifier}_y\"].append(0)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Process the keypoints\n",
    "    T = frame_count  # Number of frames processed\n",
    "    num_keypoints = len(body_identifiers)\n",
    "    keypoints_all_frames = np.empty((T, num_keypoints, 2))\n",
    "\n",
    "    for index, identifier in enumerate(body_identifiers):\n",
    "        x_key = identifier + \"_x\"\n",
    "        y_key = identifier + \"_y\"\n",
    "        x_array = keypoint_data.get(x_key, [0]*T)\n",
    "        y_array = keypoint_data.get(y_key, [0]*T)\n",
    "        data_keypoint_preprocess_x = curl_skeleton(x_array)\n",
    "        data_keypoint_preprocess_y = curl_skeleton(y_array)\n",
    "        keypoints_all_frames[:, index, 0] = np.asarray(data_keypoint_preprocess_x)\n",
    "        keypoints_all_frames[:, index, 1] = np.asarray(data_keypoint_preprocess_y)\n",
    "\n",
    "    # Draw the keypoints on black background and save images\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    image_size = (480, 640, 3)  # Height x Width x Channels\n",
    "\n",
    "    for idx in range(T):\n",
    "        black_image = np.zeros(image_size, dtype=np.uint8)\n",
    "        keypoints = keypoints_all_frames[idx]\n",
    "\n",
    "        # Reconstruct the landmarks\n",
    "        left_hand_landmarks_list = []\n",
    "        right_hand_landmarks_list = []\n",
    "        pose_landmarks_list = []\n",
    "\n",
    "        # Left hand\n",
    "        for i in range(len(hand_landmarks)):\n",
    "            x = keypoints[i + len(hand_landmarks), 0]\n",
    "            y = keypoints[i + len(hand_landmarks), 1]\n",
    "            left_hand_landmarks_list.append(\n",
    "                mp.framework.formats.landmark_pb2.NormalizedLandmark(x=x, y=y))\n",
    "\n",
    "        # Right hand\n",
    "        for i in range(len(hand_landmarks)):\n",
    "            x = keypoints[i, 0]\n",
    "            y = keypoints[i, 1]\n",
    "            right_hand_landmarks_list.append(\n",
    "                mp.framework.formats.landmark_pb2.NormalizedLandmark(x=x, y=y))\n",
    "\n",
    "        # Pose landmarks\n",
    "        for i in range(len(POSE_IDENTIFIERS)):\n",
    "            x = keypoints[2 * len(hand_landmarks) + i, 0]\n",
    "            y = keypoints[2 * len(hand_landmarks) + i, 1]\n",
    "            pose_landmarks_list.append(\n",
    "                mp.framework.formats.landmark_pb2.NormalizedLandmark(x=x, y=y))\n",
    "\n",
    "        # Create LandmarkList objects\n",
    "        left_hand_landmarks = mp.framework.formats.landmark_pb2.NormalizedLandmarkList(\n",
    "            landmark=left_hand_landmarks_list)\n",
    "        right_hand_landmarks = mp.framework.formats.landmark_pb2.NormalizedLandmarkList(\n",
    "            landmark=right_hand_landmarks_list)\n",
    "        pose_landmarks = mp.framework.formats.landmark_pb2.NormalizedLandmarkList(\n",
    "            landmark=pose_landmarks_list)\n",
    "\n",
    "        # Draw landmarks on the black image\n",
    "        mp_drawing.draw_landmarks(\n",
    "            black_image,\n",
    "            left_hand_landmarks,\n",
    "            mp_holistic.HAND_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2),\n",
    "            mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2))\n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "            black_image,\n",
    "            right_hand_landmarks,\n",
    "            mp_holistic.HAND_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "            mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2))\n",
    "\n",
    "        # Draw pose landmarks (custom connections)\n",
    "        # Since we're only using shoulders and elbows, we'll define custom connections\n",
    "        pose_connections = [\n",
    "            (0, 2),  # RIGHT_SHOULDER to RIGHT_ELBOW\n",
    "            (1, 3),  # LEFT_SHOULDER to LEFT_ELBOW\n",
    "        ]\n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "            black_image,\n",
    "            pose_landmarks,\n",
    "            pose_connections,\n",
    "            mp_drawing.DrawingSpec(color=(0, 255, 255), thickness=2, circle_radius=2),\n",
    "            mp_drawing.DrawingSpec(color=(255, 255, 0), thickness=2, circle_radius=2))\n",
    "\n",
    "        # Save image\n",
    "        output_file = os.path.join(save_dir, f\"frame_{idx:05d}.png\")\n",
    "        cv2.imwrite(output_file, black_image)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"path_to_your_video.mp4\"  # Replace with your video path\n",
    "    save_directory = \"path_to_save_directory\"  # Replace with your desired save directory\n",
    "    process_video(video_path, save_directory)\n",
    "    print(\"Processing completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of trying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ibmelab/Documents/GG/VSLRecognition/vsl/AAGCN\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_index(array):\n",
    "    for i, num in enumerate(array):\n",
    "        if num != 0:\n",
    "            return i\n",
    "\n",
    "def curl_skeleton(array):\n",
    "    if sum(array) == 0:\n",
    "        return array\n",
    "    for i, location in enumerate(array):\n",
    "        if location != 0:\n",
    "            continue\n",
    "        else:\n",
    "            if i == 0 or i == len(array) - 1:\n",
    "                continue\n",
    "            else:\n",
    "                if array[i + 1] != 0:\n",
    "                    array[i] = float((array[i - 1] + array[i + 1]) / 2)\n",
    "                else:\n",
    "                    if sum(array[i:]) == 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        j = find_index(array[i + 1:])\n",
    "                        array[i] = float(((1 + j) * array[i - 1] + 1 * array[i + 1 + j]) / (2 + j))\n",
    "    return array\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hand_landmarks = [\n",
    "        'INDEX_FINGER_DIP', 'INDEX_FINGER_MCP', 'INDEX_FINGER_PIP', 'INDEX_FINGER_TIP', \n",
    "        'MIDDLE_FINGER_DIP', 'MIDDLE_FINGER_MCP', 'MIDDLE_FINGER_PIP', 'MIDDLE_FINGER_TIP', \n",
    "        'PINKY_DIP', 'PINKY_MCP', 'PINKY_PIP', 'PINKY_TIP', \n",
    "        'RING_FINGER_DIP', 'RING_FINGER_MCP', 'RING_FINGER_PIP', 'RING_FINGER_TIP', \n",
    "        'THUMB_CMC', 'THUMB_IP', 'THUMB_MCP', 'THUMB_TIP', 'WRIST'\n",
    "    ]\n",
    "    \n",
    "    HAND_IDENTIFIERS = [id + \"_right\" for id in hand_landmarks] + [id + \"_left\" for id in hand_landmarks]\n",
    "    POSE_IDENTIFIERS = [\"RIGHT_SHOULDER\", \"LEFT_SHOULDER\", \"LEFT_ELBOW\", \"RIGHT_ELBOW\"]\n",
    "    body_identifiers = HAND_IDENTIFIERS + POSE_IDENTIFIERS \n",
    "    \n",
    "    frames = 80\n",
    "    modes = [\"train\", \"val\", \"test\"]\n",
    "    \n",
    "    for mode in modes:\n",
    "        print(f\"Processing {mode}_set.csv\")\n",
    "        train_data = pd.read_csv(f\"{mode}_set.csv\")\n",
    "        print(len(train_data))\n",
    "        \n",
    "        data = []\n",
    "        labels = []\n",
    "        \n",
    "        for video_index, video in tqdm(train_data.iterrows(), total=train_data.shape[0]):  # Ensure tqdm knows total count\n",
    "            # Remove the print statement for row_index\n",
    "            # row_index = video[\"video_path\"]\n",
    "            # print(row_index)\n",
    "\n",
    "            T = len(ast.literal_eval(video[\"INDEX_FINGER_DIP_right_x\"]))\n",
    "            current_row = np.empty(shape=(2, T, len(body_identifiers), 1))\n",
    "            for index, identifier in enumerate(body_identifiers):\n",
    "                data_keypoint_preprocess_x = curl_skeleton(ast.literal_eval(video[identifier + \"_x\"]))\n",
    "                current_row[0, :, index, :] = np.asarray(data_keypoint_preprocess_x).reshape(T, 1)\n",
    "                data_keypoint_preprocess_y = curl_skeleton(ast.literal_eval(video[identifier + \"_y\"]))\n",
    "                current_row[1, :, index, :] = np.asarray(data_keypoint_preprocess_y).reshape(T, 1)\n",
    "\n",
    "            if T < frames:\n",
    "                target = np.zeros(shape=(2, frames, len(body_identifiers), 1))\n",
    "                target[:, :T, :, :] = current_row\n",
    "            else:\n",
    "                target = current_row[:, :frames, :, :]\n",
    "                \n",
    "            data.append(target)\n",
    "            labels.append(int(video[\"label\"]))\n",
    "\n",
    "        keypoint_data = np.stack(data, axis=0)\n",
    "        label_data = np.stack(labels, axis=0)\n",
    "        np.save(f'vsl199_{mode}_right_data_preprocess.npy', keypoint_data)\n",
    "        np.save(f'vsl199_{mode}_right_label_preprocess.npy', label_data)\n",
    "        print(\"Processed and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load('vsl199_test_right_label_preprocess.npy')\n",
    "data_long = data.astype('int64')\n",
    "np.save('test_label_preprocess.npy', data_long)\n",
    "\n",
    "data = np.load('vsl199_train_right_label_preprocess.npy')\n",
    "data_long = data.astype('int64')\n",
    "np.save('train_label_preprocess.npy', data_long)\n",
    "\n",
    "data = np.load('vsl199_val_right_label_preprocess.npy')\n",
    "data_long = data.astype('int64')\n",
    "np.save('val_label_preprocess.npy', data_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ibmelab/Documents/GG/VSLRecognition/AUTSL/AAGCN\n",
    "import pandas as pd\n",
    "df = pd.read_csv('full_labels.csv')\n",
    "unique_labels = df['label'].unique()\n",
    "num_labels = len(unique_labels)\n",
    "print(num_labels)\n",
    "\n",
    "unique_labels.sort()\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Thay 'path_to_your_folder' bằng đường dẫn đến thư mục chứa video của bạn\n",
    "video_folder = '/home/ibmelab/Documents/GG/VSLRecognition/vsl/videos'\n",
    "\n",
    "video_names = []\n",
    "frame_counts = []\n",
    "\n",
    "# Duyệt qua tất cả các file trong thư mục\n",
    "for filename in os.listdir(video_folder):\n",
    "    # Kiểm tra nếu file là video (có thể mở rộng điều kiện này)\n",
    "    if filename.endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "        video_path = os.path.join(video_folder, filename)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(f\"Không thể mở video: {filename}\")\n",
    "            continue\n",
    "        \n",
    "        # Lấy số frame của video\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_counts.append(frame_count)\n",
    "        video_names.append(filename)\n",
    "        cap.release()\n",
    "        print(f\"Video: {filename}, Số frame: {frame_count}\")\n",
    "\n",
    "# Tìm số frame ít nhất và video tương ứng\n",
    "if frame_counts:\n",
    "    min_frame_count = min(frame_counts)\n",
    "    min_indices = [i for i, count in enumerate(frame_counts) if count == min_frame_count]\n",
    "    print(\"\\nSố frame ít nhất là:\", min_frame_count)\n",
    "    print(\"Video(s) có số frame ít nhất:\")\n",
    "    for idx in min_indices:\n",
    "        print(f\"- {video_names[idx]}\")\n",
    "else:\n",
    "    print(\"Không có video nào được tìm thấy trong thư mục.\")\n",
    "\n",
    "# Sau khi tính toán frame_counts\n",
    "max_frame_count = max(frame_counts)\n",
    "\n",
    "# Vẽ biểu đồ histogram của số frame\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(frame_counts, bins=30, edgecolor='black', range=(0, max_frame_count))\n",
    "plt.title('Phân bố số frame của các video')\n",
    "plt.xlabel('Số frame')\n",
    "plt.ylabel('Số lượng video')\n",
    "plt.grid(True)\n",
    "\n",
    "# Giới hạn trục x từ 0 đến số frame tối đa\n",
    "plt.xlim(0, max_frame_count)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "from feeder import FeederINCLUDE\n",
    "from aagcn import AAGCN\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import wandb\n",
    "from augumentation import Rotate, Left, Right, GaussianNoise, Compose\n",
    "from torch.utils.data import random_split\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Hyper parameter tuning : batch_size, learning_rate, weight_decay\n",
    "    #batch_size: 2->1\n",
    "    config = {'batch_size': 170, 'learning_rate': 0.0137296, 'weight_decay': 0.000150403}\n",
    "    # Load device\n",
    "    device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # Load model\n",
    "\n",
    "    # num_class: 101 -> 3 (= number of labels)\n",
    "    model = AAGCN(num_class=num_labels, num_point=46, num_person=1, in_channels=2,\n",
    "                graph_args = {\"layout\" :\"mediapipe_two_hand\", \"strategy\": \"spatial\"},\n",
    "                learning_rate=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "    # Callback PL\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            dirpath=\"checkpoints\",\n",
    "            monitor=\"valid_loss\",\n",
    "            mode=\"min\",\n",
    "            every_n_epochs = 2,\n",
    "            filename='{epoch}-{valid_accuracy:.2f}-autsl-aagcn-smaller-model'\n",
    "        ),\n",
    "    ]\n",
    "    # Augument \n",
    "    batch_size = config[\"batch_size\"]\n",
    "    transforms = Compose([\n",
    "        Rotate(15, 80, 25, (0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # Dataset class\n",
    "    ''' Đổi tên path\n",
    "    train_dataset = FeederINCLUDE(data_path=f\"wsl100_train_data_preprocess.npy\", label_path=f\"wsl100_train_label_preprocess.npy\",\n",
    "                            transform=transforms)\n",
    "    test_dataset = FeederINCLUDE(data_path=f\"wsl100_test_data_preprocess.npy\", label_path=f\"wsl100_test_label_preprocess.npy\")\n",
    "    valid_dataset = FeederINCLUDE(data_path=f\"wsl100_valid_data_preprocess.npy\", label_path=f\"wsl100_valid_label_preprocess.npy\")\n",
    "    '''\n",
    "    %cd /home/ibmelab/Documents/GG/VSLRecognition/AUTSL/AAGCN/\n",
    "    train_dataset = FeederINCLUDE(data_path=f\"autsl_train_data_preprocess.npy\", label_path=f\"train_label_preprocess.npy\",\n",
    "                            transform=transforms)\n",
    "    test_dataset = FeederINCLUDE(data_path=f\"autsl_test_data_preprocess.npy\", label_path=f\"test_label_preprocess.npy\")\n",
    "    valid_dataset = FeederINCLUDE(data_path=f\"autsl_valid_data_preprocess.npy\", label_path=f\"valid_label_preprocess.npy\")\n",
    "\n",
    "    # DataLoader\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "    val_dataloader = DataLoader(valid_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    specific_batch = next(iter(train_dataloader))\n",
    "    print(\"Input shape \", specific_batch[0].shape)\n",
    "    print(\"Data loader success\")\n",
    "    # Trainer PL\n",
    "    %cd /home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN\n",
    "    trainer = pl.Trainer(max_epochs = 120, accelerator=\"auto\", check_val_every_n_epoch = 1, \n",
    "                       devices = 1, callbacks=callbacks)\n",
    "                    #  , logger=wandb_logger) # wandb\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "    # Test PL (When test find the right ckpt_path and comment code line 58)\n",
    "    # trainer.test(model, test_dataloader, ckpt_path=\"checkpoints/epoch=61-valid_accuracy=0.91-vsl_100-aagcn-2hand+preprocessing_keypoint+augment(v1).ckpt\", \n",
    "                # verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the NumPy file\n",
    "# array = np.load('/home/ibmelab/Documents/GG/VSLRecognition/vsl/AAGCN/vsl199_train_data_preprocess.npy')\n",
    "array = np.load('/home/ibmelab/Documents/GG/VSLRecognition/vsl/gcn_keypoints_v2/01_Co-Hien_1-100_1-2-3_0108___center_device02_signer01_center_ord1_2/hand_flow_00000.npy')\n",
    "# array = np.load('/home/ibmelab/Documents/GG/VSLRecognition/vsl/hand_keypoints/01_Co-Hien_1-100_1-2-3_0108___center_device02_signer01_center_ord1_3/hand_kp_00000.npy')\n",
    "# Print the shape of the array\n",
    "print(array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from feeder import FeederINCLUDE, FeederCustomV2\n",
    "from aagcn import AAGCN\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from augumentation import Rotate, Compose, GaussianNoise\n",
    "from pytorch_lightning.utilities.migration import pl_legacy_patch\n",
    "import os\n",
    "transforms = Compose([\n",
    "        Rotate(15, 80, 25, (0.5, 0.5))\n",
    "    ])\n",
    "# Lấy mẫu từ FeederINCLUDE\n",
    "%cd /home/ibmelab/Documents/GG/VSLRecognition/vsl/AAGCN/\n",
    "train_dataset_include = FeederINCLUDE(\n",
    "        data_path=\"vsl199_train_right_data_preprocess.npy\",\n",
    "        label_path=\"train_label_preprocess.npy\",\n",
    "        transform=transforms)\n",
    "data_include, label_include = train_dataset_include[3]\n",
    "print(f\"FeederINCLUDE sample data shape: {data_include.shape}\")\n",
    "print(f\"FeederINCLUDE sample label: {label_include}\")\n",
    "\n",
    "# Lấy mẫu từ FeederCustomV2\n",
    "train_dataset_custom = FeederCustomV2('/home/ibmelab/Documents/GG/VSLRecognition/vsl','train')\n",
    "data_custom, label_custom = train_dataset_custom[5]\n",
    "print(f\"FeederCustomV2 sample data shape: {data_custom.shape}\")\n",
    "print(f\"FeederCustomV2 sample label: {label_custom}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN/\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from feeder import FeederINCLUDE, FeederCustomV2\n",
    "from aagcn import AAGCN\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from augumentation import Rotate, Compose, GaussianNoise\n",
    "from pytorch_lightning.utilities.migration import pl_legacy_patch\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    config = {'batch_size': 160, 'learning_rate': 0.0137296, 'weight_decay': 0.000150403}\n",
    "    num_labels = 199  # Set your new number of classes here\n",
    "\n",
    "    # Load device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Initialize the model with the new number of classes\n",
    "    model = AAGCN(\n",
    "        num_class=num_labels,\n",
    "        num_point=46,\n",
    "        num_person=1,\n",
    "        in_channels=2,\n",
    "        graph_args={\"layout\": \"mediapipe_two_hand\", \"strategy\": \"spatial\"},\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    # Path to your checkpoint\n",
    "    checkpoint_path = \"checkpoints/epoch=65-valid_accuracy=0.86-autsl-aagcn-fold=0.ckpt\"\n",
    "\n",
    "    # Load the checkpoint\n",
    "    with pl_legacy_patch():\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # Get the state dict\n",
    "    state_dict = checkpoint['state_dict']\n",
    "\n",
    "    # Remove the keys for the final layer (adjust 'fc' to match your model's final layer name)\n",
    "    filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith('fc.')}\n",
    "\n",
    "    # Load the filtered state dict into the model\n",
    "    model.load_state_dict(filtered_state_dict, strict=False)\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            dirpath=\"checkpoints\",\n",
    "            monitor=\"valid_loss\",\n",
    "            mode=\"min\",\n",
    "            every_n_epochs=2,\n",
    "            filename='{epoch}-{valid_accuracy:.2f}-vsl199-model'\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    transforms = Compose([\n",
    "        Rotate(15, 80, 25, (0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "\n",
    "    %cd /home/ibmelab/Documents/GG/VSLRecognition/vsl/AAGCN/\n",
    "    # Datasets\n",
    "    # train_dataset = FeederINCLUDE(\n",
    "    #     data_path=\"vsl199_train_right_data_preprocess.npy\",\n",
    "    #     label_path=\"train_label_preprocess.npy\",\n",
    "    #     transform=transforms\n",
    "    # )\n",
    "    # test_dataset = FeederINCLUDE(\n",
    "    #     data_path=\"vsl199_test_right_data_preprocess.npy\",\n",
    "    #     label_path=\"test_label_preprocess.npy\"\n",
    "    # )\n",
    "    # valid_dataset = FeederINCLUDE(\n",
    "    #     data_path=\"vsl199_val_right_data_preprocess.npy\",\n",
    "    #     label_path=\"val_label_preprocess.npy\"\n",
    "    # )\n",
    "    def gcn_bert_collate_fn_(batch):\n",
    "        labels = torch.stack([s[1] for s in batch],dim = 0)\n",
    "        keypoints = torch.stack([s[0] for s in batch],dim = 0) # bs t n c\n",
    "                                                                                                             \n",
    "        return {'keypoints':keypoints},labels\n",
    "\n",
    "    collate_func = gcn_bert_collate_fn_\n",
    "    train_dataset = FeederCustomV2('/home/ibmelab/Documents/GG/VSLRecognition/vsl','train')\n",
    "    test_dataset = FeederCustomV2('/home/ibmelab/Documents/GG/VSLRecognition/vsl','test')\n",
    "    valid_dataset = FeederCustomV2('/home/ibmelab/Documents/GG/VSLRecognition/vsl','val')\n",
    "\n",
    "    # DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, collate_fn=collate_func,batch_size=config[\"batch_size\"], shuffle=True,\n",
    "                                  num_workers = 12, prefetch_factor = 4, persistent_workers =  True)\n",
    "    test_dataloader = DataLoader(test_dataset, collate_fn=collate_func,batch_size=config[\"batch_size\"], shuffle=False,\n",
    "                                 num_workers = 12, prefetch_factor = 4, persistent_workers =  True)\n",
    "    val_dataloader = DataLoader(valid_dataset, collate_fn=collate_func,batch_size=config[\"batch_size\"], shuffle=False,\n",
    "                                num_workers = 12, prefetch_factor = 4, persistent_workers =  True)\n",
    "\n",
    "    %cd /home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN/\n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=120,\n",
    "        accelerator=\"auto\",\n",
    "        check_val_every_n_epoch=1,\n",
    "        devices=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN/\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from feeder import FeederINCLUDE, FeederCustomV2\n",
    "from aagcn import AAGCN\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from augumentation import Rotate, Compose\n",
    "from pytorch_lightning.utilities.migration import pl_legacy_patch\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    config = {'batch_size': 90, 'learning_rate': 0.0137296, 'weight_decay': 0.000150403}\n",
    "    num_labels = 199  # Set your new number of classes here\n",
    "\n",
    "    # Load device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Initialize the model with the new number of classes\n",
    "    model = AAGCN(\n",
    "        num_class=num_labels,\n",
    "        num_point=46,\n",
    "        num_person=1,\n",
    "        in_channels=2,\n",
    "        graph_args={\"layout\": \"mediapipe_two_hand\", \"strategy\": \"spatial\"},\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    # Path to your checkpoint\n",
    "    checkpoint_path = \"checkpoints/epoch=95-valid_accuracy=0.73-vsl199.ckpt\"\n",
    "\n",
    "    # Load the checkpoint\n",
    "    with pl_legacy_patch():\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # Get the state dict\n",
    "    state_dict = checkpoint['state_dict']\n",
    "\n",
    "    # Remove the keys for the final layer (adjust 'fc' to match your model's final layer name)\n",
    "    # filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith('fc.')}\n",
    "\n",
    "    # Load the filtered state dict into the model\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            dirpath=\"checkpoints\",\n",
    "            monitor=\"valid_loss\",\n",
    "            mode=\"min\",\n",
    "            every_n_epochs=2,\n",
    "            filename='{epoch}-{valid_accuracy:.2f}-vsl199-FeederCustom'\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    transforms = Compose([\n",
    "        Rotate(15, 80, 25, (0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "\n",
    "    %cd /home/ibmelab/Documents/GG/VSLRecognition/vsl/AAGCN/\n",
    "    # Datasets\n",
    "    train_dataset = FeederINCLUDE(\n",
    "        data_path=\"vsl199_train_data_preprocess.npy\",\n",
    "        label_path=\"train_label_preprocess.npy\",\n",
    "        transform=transforms\n",
    "    )\n",
    "    test_dataset = FeederINCLUDE(\n",
    "        data_path=\"vsl199_test_data_preprocess.npy\",\n",
    "        label_path=\"test_label_preprocess.npy\"\n",
    "    )\n",
    "    valid_dataset = FeederINCLUDE(\n",
    "        data_path=\"vsl199_val_data_preprocess.npy\",\n",
    "        label_path=\"val_label_preprocess.npy\"\n",
    "    )\n",
    "    # def gcn_bert_collate_fn_(batch):\n",
    "    #     labels = torch.stack([s[1] for s in batch],dim = 0)\n",
    "    #     keypoints = torch.stack([s[0] for s in batch],dim = 0) # bs t n c\n",
    "                                                                                                             \n",
    "    #     return {'keypoints':keypoints},labels\n",
    "\n",
    "    # collate_func = gcn_bert_collate_fn_\n",
    "    # train_dataset = FeederCustomV2('/home/ibmelab/Documents/GG/VSLRecognition/vsl','train')\n",
    "    # test_dataset = FeederCustomV2('/home/ibmelab/Documents/GG/VSLRecognition/vsl','test')\n",
    "    # valid_dataset = FeederCustomV2('/home/ibmelab/Documents/GG/VSLRecognition/vsl','val')\n",
    "\n",
    "    # DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True,\n",
    "                                  num_workers = 12, prefetch_factor = 4, persistent_workers =  True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False,\n",
    "                                 num_workers = 12, prefetch_factor = 4, persistent_workers =  True)\n",
    "    val_dataloader = DataLoader(valid_dataset, batch_size=config[\"batch_size\"], shuffle=False,\n",
    "                                num_workers = 12, prefetch_factor = 4, persistent_workers =  True)\n",
    "\n",
    "    %cd /home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN/\n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=120,\n",
    "        accelerator=\"auto\",\n",
    "        check_val_every_n_epoch=1,\n",
    "        devices=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "from feeder import FeederINCLUDE\n",
    "from aagcn import AAGCN\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import wandb\n",
    "from augumentation import Rotate, Left, Right, GaussianNoise, Compose\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Hyper parameter tuning : batch_size, learning_rate, weight_decay\n",
    "    #batch_size: 2->1\n",
    "    config = {'batch_size': 170, 'learning_rate': 0.0137296, 'weight_decay': 0.000150403}\n",
    "    # Load device\n",
    "    device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # Load model\n",
    "\n",
    "    # num_class: 101 -> 3 (= number of labels)\n",
    "    model = AAGCN(num_class=199, num_point=46, num_person=1, in_channels=2,\n",
    "                graph_args = {\"layout\" :\"mediapipe_two_hand\", \"strategy\": \"spatial\"},\n",
    "                learning_rate=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "    # Callback PL\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            dirpath=\"checkpoints\",\n",
    "            monitor=\"valid_accuracy\",\n",
    "            mode=\"max\",\n",
    "            every_n_epochs = 2,\n",
    "            filename='{epoch}-{valid_accuracy:.2f}-wsl_100-aagcn-{fold}'\n",
    "        ),\n",
    "    ]\n",
    "    # Augument \n",
    "    batch_size = config[\"batch_size\"]\n",
    "    transforms = Compose([\n",
    "        Rotate(15, 80, 25, (0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # Dataset class\n",
    "    ''' Đổi tên path\n",
    "    train_dataset = FeederINCLUDE(data_path=f\"wsl100_train_data_preprocess.npy\", label_path=f\"wsl100_train_label_preprocess.npy\",\n",
    "                            transform=transforms)\n",
    "    test_dataset = FeederINCLUDE(data_path=f\"wsl100_test_data_preprocess.npy\", label_path=f\"wsl100_test_label_preprocess.npy\")\n",
    "    valid_dataset = FeederINCLUDE(data_path=f\"wsl100_valid_data_preprocess.npy\", label_path=f\"wsl100_valid_label_preprocess.npy\")\n",
    "    '''\n",
    "    %cd /home/ibmelab/Documents/GG/VSLRecognition/vsl/AAGCN/\n",
    "    # Datasets\n",
    "    train_dataset = FeederINCLUDE(\n",
    "        data_path=\"vsl199_train_right_data_preprocess.npy\",\n",
    "        label_path=\"train_label_preprocess.npy\",\n",
    "        transform=transforms\n",
    "    )\n",
    "    test_dataset = FeederINCLUDE(\n",
    "        data_path=\"vsl199_test_right_data_preprocess.npy\",\n",
    "        label_path=\"test_label_preprocess.npy\"\n",
    "    )\n",
    "    valid_dataset = FeederINCLUDE(\n",
    "        data_path=\"vsl199_val_right_data_preprocess.npy\",\n",
    "        label_path=\"val_label_preprocess.npy\"\n",
    "    )\n",
    "\n",
    "    # DataLoader\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "    val_dataloader = DataLoader(valid_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    specific_batch = next(iter(train_dataloader))\n",
    "    print(\"Input shape \", specific_batch[0].shape)\n",
    "    print(\"Data loader success\")\n",
    "    # Trainer PL\n",
    "    %cd /home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN\n",
    "    trainer = pl.Trainer(max_epochs = 120, accelerator=\"auto\", check_val_every_n_epoch = 1, \n",
    "                       devices = 1, callbacks=callbacks)\n",
    "                    #  , logger=wandb_logger) # wandb\n",
    "    #trainer.fit(model, train_dataloader, val_dataloader)\n",
    "    # Test PL (When test find the right ckpt_path and comment code line 58)\n",
    "    trainer.test(model, test_dataloader, ckpt_path=\"checkpoints/epoch=53-valid_accuracy=0.62-vsl199-small-model.ckpt\", \n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "son",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
