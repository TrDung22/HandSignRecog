{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ibmelab/Documents/GG/VSLRecognition/AUTSL/AAGCN/\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "\n",
    "#folder_path = r'dataset'  # Use raw string to avoid issues with backslashes\n",
    "#folder_path = r'30 class 28 actor (center)'\n",
    "folder_path = r'/home/ibmelab/Documents/GG/VSLRecognition/AUTSL/videos'\n",
    "csv_file_path = 'videos_list.csv'\n",
    "labels_file_path = '1_1000_label.csv'\n",
    "final_file_path = 'temp_videos_list.csv'\n",
    "\n",
    "# Read the label and gloss mapping\n",
    "label_to_gloss = {}\n",
    "with open(labels_file_path, mode='r', encoding='utf-8') as labels_file:\n",
    "    csv_reader = csv.DictReader(labels_file)\n",
    "    for row in csv_reader:\n",
    "        label = int(row['id_label_in_documents'])\n",
    "        gloss = row['name']\n",
    "        label_to_gloss[label] = gloss\n",
    "\n",
    "# Write video names, labels, and glosses to a CSV file\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['file', 'label', 'gloss'])\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.mp4', '.mkv', '.avi', '.mov', '.flv', '.wmv')):\n",
    "            match = re.search(r'_(\\d+)\\.', filename)\n",
    "            if match:\n",
    "                label = int(match.group(1))\n",
    "                gloss = label_to_gloss.get(label, 'Unknown')\n",
    "            else:\n",
    "                label = 'N/A'\n",
    "                gloss = 'Unknown'\n",
    "\n",
    "            full_filename = os.path.join(folder_path, filename)\n",
    "            csv_writer.writerow([full_filename, label, gloss])\n",
    "\n",
    "print(f'Video names have been written to {csv_file_path}')\n",
    "\n",
    "# Find min label\n",
    "with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    labels = [int(row[\"label\"]) for row in csv_reader if row[\"label\"].isdigit()]  # Convert to int and filter out 'N/A'\n",
    "    min_label = min(labels) if labels else None\n",
    "\n",
    "print(\"Minimum label:\", min_label)\n",
    "\n",
    "# Normalize labels\n",
    "with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csv_file, \\\n",
    "     open(final_file_path, mode='w', newline='', encoding='utf-8') as final_file:\n",
    "    \n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    fieldnames = csv_reader.fieldnames\n",
    "    \n",
    "    csv_writer = csv.DictWriter(final_file, fieldnames=fieldnames)\n",
    "    csv_writer.writeheader()\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        if row['label'].isdigit():  # Check if label is a digit before converting\n",
    "            row['label'] = str(int(row['label']) - min_label)  # Normalize and convert back to string\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "# Replace the original file with the updated file\n",
    "os.replace(final_file_path, csv_file_path)\n",
    "\n",
    "print(\"Labels have been updated and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ibmelab/Documents/GG/VSLRecognition/vsl/AAGCN\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ibmelab/Documents/GG/VSLRecognition/vsl/AAGCN/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "\n",
    "videos_df = pd.read_csv('videos_list.csv')\n",
    "videos_df['label'] = videos_df['file'].apply(lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "def extract_actor_name(file_name):\n",
    "    parts = file_name.split('_')\n",
    "    if len(parts) > 1:\n",
    "        return '_'.join(parts[:2])\n",
    "    return file_name.strip()\n",
    "\n",
    "videos_df['actor'] = videos_df['file'].apply(lambda x: extract_actor_name(x.split('\\\\')[-1]))\n",
    "\n",
    "#print(videos_df['actor'])\n",
    "\n",
    "unique_actors = videos_df['actor'].unique()\n",
    "train_actors = random.sample(list(unique_actors), 24)\n",
    "remaining_actors = [actor for actor in unique_actors if actor not in train_actors]\n",
    "valid_actors = random.sample(remaining_actors, 2)\n",
    "test_actors = [actor for actor in remaining_actors if actor not in valid_actors]\n",
    "\n",
    "train_videos = videos_df[videos_df['actor'].isin(train_actors)][['file', 'label']]\n",
    "valid_videos = videos_df[videos_df['actor'].isin(valid_actors)][['file', 'label']]\n",
    "test_videos = videos_df[videos_df['actor'].isin(test_actors)][['file', 'label']]\n",
    "\n",
    "train_videos.to_csv('train.csv', index=False)\n",
    "valid_videos.to_csv('valid.csv', index=False)\n",
    "test_videos.to_csv('test.csv', index=False)\n",
    "\n",
    "unique_actors.sort()\n",
    "#print(unique_actors)\n",
    "\n",
    "print(f'Train videos: {len(train_videos)}')\n",
    "print(f'Validation videos: {len(valid_videos)}')\n",
    "print(f'Test videos: {len(test_videos)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hand_landmarks = ['INDEX_FINGER_DIP', 'INDEX_FINGER_MCP', 'INDEX_FINGER_PIP', 'INDEX_FINGER_TIP', \n",
    "                  'MIDDLE_FINGER_DIP', 'MIDDLE_FINGER_MCP', 'MIDDLE_FINGER_PIP', 'MIDDLE_FINGER_TIP', \n",
    "                  'PINKY_DIP', 'PINKY_MCP', 'PINKY_PIP', 'PINKY_TIP', 'RING_FINGER_DIP', 'RING_FINGER_MCP', \n",
    "                  'RING_FINGER_PIP', 'RING_FINGER_TIP', 'THUMB_CMC', 'THUMB_IP', 'THUMB_MCP', 'THUMB_TIP', 'WRIST']\n",
    "pose_landmarks = ['LEFT_ANKLE', 'LEFT_EAR', 'LEFT_ELBOW', 'LEFT_EYE', 'LEFT_EYE_INNER', 'LEFT_EYE_OUTER', \n",
    "                  'LEFT_FOOT_INDEX', 'LEFT_HEEL', 'LEFT_HIP', 'LEFT_INDEX', 'LEFT_KNEE', 'LEFT_PINKY', \n",
    "                  'LEFT_SHOULDER', 'LEFT_THUMB', 'LEFT_WRIST', 'MOUTH_LEFT', 'MOUTH_RIGHT', 'NOSE', \n",
    "                  'RIGHT_ANKLE', 'RIGHT_EAR', 'RIGHT_ELBOW', 'RIGHT_EYE', 'RIGHT_EYE_INNER', 'RIGHT_EYE_OUTER', \n",
    "                  'RIGHT_FOOT_INDEX', 'RIGHT_HEEL', 'RIGHT_HIP', 'RIGHT_INDEX', 'RIGHT_KNEE', 'RIGHT_PINKY', \n",
    "                  'RIGHT_SHOULDER', 'RIGHT_THUMB', 'RIGHT_WRIST']\n",
    "\n",
    "def extract_keypoint(video_path, label):\n",
    "    video =   f\"/home/ibmelab/Documents/GG/VSLRecognition/vsl/videos/{video_path}\"\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    \n",
    "    keypoint_dict = defaultdict(list)\n",
    "    count = 0\n",
    "\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            count += 1\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = holistic.process(image)\n",
    "\n",
    "            if results.right_hand_landmarks:\n",
    "                for idx, landmark in enumerate(results.right_hand_landmarks.landmark): \n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_right_x\"].append(landmark.x)\n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_right_y\"].append(landmark.y)\n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_right_z\"].append(landmark.z)\n",
    "            else:\n",
    "                for idx in range(len(hand_landmarks)):\n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_right_x\"].append(0)\n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_right_y\"].append(0)\n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_right_z\"].append(0)\n",
    "\n",
    "            if results.left_hand_landmarks:\n",
    "                for idx, landmark in enumerate(results.left_hand_landmarks.landmark): \n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_left_x\"].append(landmark.x)\n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_left_y\"].append(landmark.y)\n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_left_z\"].append(landmark.z)\n",
    "            else:\n",
    "                for idx in range(len(hand_landmarks)):\n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_left_x\"].append(0)\n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_left_y\"].append(0)\n",
    "                    keypoint_dict[f\"{hand_landmarks[idx]}_left_z\"].append(0)\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "                for idx, landmark in enumerate(results.pose_landmarks.landmark): \n",
    "                    keypoint_dict[f\"{pose_landmarks[idx]}_x\"].append(landmark.x)\n",
    "                    keypoint_dict[f\"{pose_landmarks[idx]}_y\"].append(landmark.y)\n",
    "                    keypoint_dict[f\"{pose_landmarks[idx]}_z\"].append(landmark.z)\n",
    "            else:\n",
    "                for idx in range(len(pose_landmarks)):\n",
    "                    keypoint_dict[f\"{pose_landmarks[idx]}_x\"].append(0)\n",
    "                    keypoint_dict[f\"{pose_landmarks[idx]}_y\"].append(0)\n",
    "                    keypoint_dict[f\"{pose_landmarks[idx]}_z\"].append(0)\n",
    "\n",
    "        keypoint_dict[\"frame\"] = count\n",
    "        keypoint_dict[\"video_path\"] = video_path\n",
    "        keypoint_dict[\"label\"] = label\n",
    "\n",
    "        return keypoint_dict\n",
    "\n",
    "def process_videos(mode):\n",
    "    csv_file = f\"/home/ibmelab/Documents/GG/VSLRecognition/vsl/label1-200/label/labelThreeView/labelThreeViewTesting/{mode}_labels.csv\"\n",
    "    data = pd.read_csv(csv_file)\n",
    "\n",
    "    keypoints_list = Parallel(n_jobs=-1)(  # Chạy song song với số lượng core tối đa\n",
    "        delayed(extract_keypoint)(row['file_name'], row['label_id']) for index, row in data.iterrows()\n",
    "    )\n",
    "\n",
    "    # Tạo DataFrame và lưu vào CSV\n",
    "    keypoints_df = pd.DataFrame(keypoints_list)\n",
    "    keypoints_df.to_csv(f\"{mode}_3views_set.csv\", index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    modes = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "    for mode in modes:\n",
    "        process_videos(mode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try something new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ibmelab/Documents/GG/VSLRecognition/vsl\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "def find_index(array):\n",
    "    for i, num in enumerate(array):\n",
    "        if num != 0:\n",
    "            return i\n",
    "    return -1  # Return -1 if no non-zero element is found\n",
    "\n",
    "def curl_skeleton(array):\n",
    "    if sum(array) == 0:\n",
    "        return array\n",
    "    for i, location in enumerate(array):\n",
    "        if location != 0:\n",
    "            continue\n",
    "        else:\n",
    "            if i == 0 or i == len(array) - 1:\n",
    "                continue\n",
    "            else:\n",
    "                if array[i + 1] != 0:\n",
    "                    array[i] = float((array[i - 1] + array[i + 1]) / 2)\n",
    "                else:\n",
    "                    j = find_index(array[i + 1:])\n",
    "                    if j == -1:\n",
    "                        continue\n",
    "                    array[i] = float(((1 + j) * array[i - 1] + array[i + 1 + j]) / (2 + j))\n",
    "    return array\n",
    "\n",
    "def impute_missing_keypoints(poses):\n",
    "    \"\"\"Replace missing keypoints (zeros) with values from neighboring frames.\"\"\"\n",
    "    poses_imputed = poses.copy()\n",
    "    num_frames, num_keypoints, _ = poses.shape\n",
    "    for kpi in range(num_keypoints):\n",
    "        # Find frames where keypoint is missing\n",
    "        missing = np.where(np.all(poses[:, kpi, :] == 0, axis=1))[0]\n",
    "        for idx in missing:\n",
    "            # Find the nearest frame where the keypoint is not missing\n",
    "            non_missing = np.where(np.any(poses[:, kpi, :] != 0, axis=1))[0]\n",
    "            if len(non_missing) == 0:\n",
    "                continue\n",
    "            nearest = non_missing[np.argmin(np.abs(non_missing - idx))]\n",
    "            poses_imputed[idx, kpi, :] = poses[nearest, kpi, :]\n",
    "    return poses_imputed\n",
    "\n",
    "def normalize_keypoints(poses):\n",
    "    \"\"\"Normalize keypoints to be in the range [0,1].\"\"\"\n",
    "    min_vals = poses.min(axis=(0,1))\n",
    "    max_vals = poses.max(axis=(0,1))\n",
    "    range_vals = max_vals - min_vals\n",
    "    range_vals[range_vals == 0] = 1  # Avoid division by zero\n",
    "    poses_normalized = (poses - min_vals) / range_vals\n",
    "    return poses_normalized\n",
    "\n",
    "def calc_pose_flow(prev, current):\n",
    "    \"\"\"Calculate the flow (angle and magnitude) between two frames.\"\"\"\n",
    "    result = np.zeros_like(prev)  # Shape: (num_keypoints, 2)\n",
    "    for kpi in range(prev.shape[0]):\n",
    "        if np.all(prev[kpi] == 0) or np.all(current[kpi] == 0):\n",
    "            result[kpi] = 0.0\n",
    "            continue\n",
    "\n",
    "        dx = current[kpi, 0] - prev[kpi, 0]\n",
    "        dy = current[kpi, 1] - prev[kpi, 1]\n",
    "        ang = math.atan2(dy, dx)\n",
    "        mag = np.hypot(dx, dy)\n",
    "\n",
    "        result[kpi, 0] = ang\n",
    "        result[kpi, 1] = mag\n",
    "\n",
    "    return result  # Shape: (num_keypoints, 2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hand_landmarks = [\n",
    "        'INDEX_FINGER_DIP', 'INDEX_FINGER_MCP', 'INDEX_FINGER_PIP', 'INDEX_FINGER_TIP',\n",
    "        'MIDDLE_FINGER_DIP', 'MIDDLE_FINGER_MCP', 'MIDDLE_FINGER_PIP', 'MIDDLE_FINGER_TIP',\n",
    "        'PINKY_DIP', 'PINKY_MCP', 'PINKY_PIP', 'PINKY_TIP',\n",
    "        'RING_FINGER_DIP', 'RING_FINGER_MCP', 'RING_FINGER_PIP', 'RING_FINGER_TIP',\n",
    "        'THUMB_CMC', 'THUMB_IP', 'THUMB_MCP', 'THUMB_TIP', 'WRIST'\n",
    "    ]\n",
    "\n",
    "    HAND_IDENTIFIERS = [id + \"_right\" for id in hand_landmarks] + [id + \"_left\" for id in hand_landmarks]\n",
    "    POSE_IDENTIFIERS = [\"RIGHT_SHOULDER\", \"LEFT_SHOULDER\", \"LEFT_ELBOW\", \"RIGHT_ELBOW\"]\n",
    "    body_identifiers = HAND_IDENTIFIERS + POSE_IDENTIFIERS\n",
    "\n",
    "    modes = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "    output_folder = \"gcn_keypoints_v2\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for mode in modes:\n",
    "        print(f\"Processing {mode}_3views_set.csv\")\n",
    "        dataset = pd.read_csv(f\"AAGCN/{mode}_3views_set.csv\")\n",
    "        print(f\"Number of videos in {mode} set: {len(dataset)}\")\n",
    "\n",
    "        for video_index, video in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "            video_name = video[\"video_path\"]  # Assuming there's a 'video_path' column\n",
    "            video_base = os.path.splitext(os.path.basename(video_name))[0]  # Get the base name of the video\n",
    "\n",
    "            T = len(ast.literal_eval(video[\"INDEX_FINGER_DIP_right_x\"]))\n",
    "            num_keypoints = len(body_identifiers)\n",
    "            keypoints_all_frames = np.zeros((T, num_keypoints, 2))\n",
    "\n",
    "            for index, identifier in enumerate(body_identifiers):\n",
    "                data_keypoint_preprocess_x = curl_skeleton(ast.literal_eval(video[identifier + \"_x\"]))\n",
    "                data_keypoint_preprocess_y = curl_skeleton(ast.literal_eval(video[identifier + \"_y\"]))\n",
    "                keypoints_all_frames[:, index, 0] = np.asarray(data_keypoint_preprocess_x)\n",
    "                keypoints_all_frames[:, index, 1] = np.asarray(data_keypoint_preprocess_y)\n",
    "\n",
    "            # Impute missing keypoints\n",
    "            keypoints_all_frames = impute_missing_keypoints(keypoints_all_frames)\n",
    "\n",
    "            # Normalize keypoints\n",
    "            keypoints_all_frames = normalize_keypoints(keypoints_all_frames)\n",
    "\n",
    "            # Create output folder for the video\n",
    "            video_output_folder = os.path.join(output_folder, video_base)\n",
    "            os.makedirs(video_output_folder, exist_ok=True)\n",
    "\n",
    "            # Compute flow and save per frame\n",
    "            prev_frame = keypoints_all_frames[0]\n",
    "            for idx in range(1, len(keypoints_all_frames)):\n",
    "                current_frame = keypoints_all_frames[idx]\n",
    "                flow = calc_pose_flow(prev_frame, current_frame)\n",
    "                # Save the flow data per frame\n",
    "                output_file = os.path.join(video_output_folder, f\"hand_flow_{(idx-1):05d}.npy\")\n",
    "                np.save(output_file, flow)\n",
    "                prev_frame = current_frame\n",
    "\n",
    "        print(f\"Processing of {mode} set completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of trying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_index(array):\n",
    "    for i, num in enumerate(array):\n",
    "        if num != 0:\n",
    "            return i\n",
    "\n",
    "def curl_skeleton(array):\n",
    "    if sum(array) == 0:\n",
    "        return array\n",
    "    for i, location in enumerate(array):\n",
    "        if location != 0:\n",
    "            continue\n",
    "        else:\n",
    "            if i == 0 or i == len(array) - 1:\n",
    "                continue\n",
    "            else:\n",
    "                if array[i + 1] != 0:\n",
    "                    array[i] = float((array[i - 1] + array[i + 1]) / 2)\n",
    "                else:\n",
    "                    if sum(array[i:]) == 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        j = find_index(array[i + 1:])\n",
    "                        array[i] = float(((1 + j) * array[i - 1] + 1 * array[i + 1 + j]) / (2 + j))\n",
    "    return array\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hand_landmarks = [\n",
    "        'INDEX_FINGER_DIP', 'INDEX_FINGER_MCP', 'INDEX_FINGER_PIP', 'INDEX_FINGER_TIP', \n",
    "        'MIDDLE_FINGER_DIP', 'MIDDLE_FINGER_MCP', 'MIDDLE_FINGER_PIP', 'MIDDLE_FINGER_TIP', \n",
    "        'PINKY_DIP', 'PINKY_MCP', 'PINKY_PIP', 'PINKY_TIP', \n",
    "        'RING_FINGER_DIP', 'RING_FINGER_MCP', 'RING_FINGER_PIP', 'RING_FINGER_TIP', \n",
    "        'THUMB_CMC', 'THUMB_IP', 'THUMB_MCP', 'THUMB_TIP', 'WRIST'\n",
    "    ]\n",
    "    \n",
    "    HAND_IDENTIFIERS = [id + \"_right\" for id in hand_landmarks] + [id + \"_left\" for id in hand_landmarks]\n",
    "    POSE_IDENTIFIERS = [\"RIGHT_SHOULDER\", \"LEFT_SHOULDER\", \"LEFT_ELBOW\", \"RIGHT_ELBOW\"]\n",
    "    body_identifiers = HAND_IDENTIFIERS + POSE_IDENTIFIERS \n",
    "    \n",
    "    frames = 80\n",
    "    modes = [\"train\", \"val\", \"test\"]\n",
    "    \n",
    "    for mode in modes:\n",
    "        print(f\"Processing {mode}_set.csv\")\n",
    "        train_data = pd.read_csv(f\"{mode}_set.csv\")\n",
    "        print(len(train_data))\n",
    "        \n",
    "        data = []\n",
    "        labels = []\n",
    "        \n",
    "        for video_index, video in tqdm(train_data.iterrows(), total=train_data.shape[0]):  # Ensure tqdm knows total count\n",
    "            # Remove the print statement for row_index\n",
    "            # row_index = video[\"video_path\"]\n",
    "            # print(row_index)\n",
    "\n",
    "            T = len(ast.literal_eval(video[\"INDEX_FINGER_DIP_right_x\"]))\n",
    "            current_row = np.empty(shape=(2, T, len(body_identifiers), 1))\n",
    "            for index, identifier in enumerate(body_identifiers):\n",
    "                data_keypoint_preprocess_x = curl_skeleton(ast.literal_eval(video[identifier + \"_x\"]))\n",
    "                current_row[0, :, index, :] = np.asarray(data_keypoint_preprocess_x).reshape(T, 1)\n",
    "                data_keypoint_preprocess_y = curl_skeleton(ast.literal_eval(video[identifier + \"_y\"]))\n",
    "                current_row[1, :, index, :] = np.asarray(data_keypoint_preprocess_y).reshape(T, 1)\n",
    "\n",
    "            if T < frames:\n",
    "                target = np.zeros(shape=(2, frames, len(body_identifiers), 1))\n",
    "                target[:, :T, :, :] = current_row\n",
    "            else:\n",
    "                target = current_row[:, :frames, :, :]\n",
    "                \n",
    "            data.append(target)\n",
    "            labels.append(int(video[\"label\"]))\n",
    "\n",
    "        keypoint_data = np.stack(data, axis=0)\n",
    "        label_data = np.stack(labels, axis=0)\n",
    "        np.save(f'vsl199_{mode}_data_preprocess.npy', keypoint_data)\n",
    "        np.save(f'vsl199_{mode}_label_preprocess.npy', label_data)\n",
    "        print(\"Processed and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load('vsl199_test_label_preprocess.npy')\n",
    "data_long = data.astype('int64')\n",
    "np.save('test_label_preprocess.npy', data_long)\n",
    "\n",
    "data = np.load('vsl199_train_label_preprocess.npy')\n",
    "data_long = data.astype('int64')\n",
    "np.save('train_label_preprocess.npy', data_long)\n",
    "\n",
    "data = np.load('vsl199_val_label_preprocess.npy')\n",
    "data_long = data.astype('int64')\n",
    "np.save('val_label_preprocess.npy', data_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ibmelab/Documents/GG/VSLRecognition/AUTSL/AAGCN\n",
    "import pandas as pd\n",
    "df = pd.read_csv('full_labels.csv')\n",
    "unique_labels = df['label'].unique()\n",
    "num_labels = len(unique_labels)\n",
    "print(num_labels)\n",
    "\n",
    "unique_labels.sort()\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "from feeder import FeederINCLUDE\n",
    "from aagcn import AAGCN\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import wandb\n",
    "from augumentation import Rotate, Left, Right, GaussianNoise, Compose\n",
    "from torch.utils.data import random_split\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Hyper parameter tuning : batch_size, learning_rate, weight_decay\n",
    "    #batch_size: 2->1\n",
    "    config = {'batch_size': 170, 'learning_rate': 0.0137296, 'weight_decay': 0.000150403}\n",
    "    # Load device\n",
    "    device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # Load model\n",
    "\n",
    "    # num_class: 101 -> 3 (= number of labels)\n",
    "    model = AAGCN(num_class=num_labels, num_point=46, num_person=1, in_channels=2,\n",
    "                graph_args = {\"layout\" :\"mediapipe_two_hand\", \"strategy\": \"spatial\"},\n",
    "                learning_rate=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "    # Callback PL\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            dirpath=\"checkpoints\",\n",
    "            monitor=\"valid_loss\",\n",
    "            mode=\"min\",\n",
    "            every_n_epochs = 2,\n",
    "            filename='{epoch}-{valid_accuracy:.2f}-autsl-aagcn-smaller-model'\n",
    "        ),\n",
    "    ]\n",
    "    # Augument \n",
    "    batch_size = config[\"batch_size\"]\n",
    "    transforms = Compose([\n",
    "        Rotate(15, 80, 25, (0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # Dataset class\n",
    "    ''' Đổi tên path\n",
    "    train_dataset = FeederINCLUDE(data_path=f\"wsl100_train_data_preprocess.npy\", label_path=f\"wsl100_train_label_preprocess.npy\",\n",
    "                            transform=transforms)\n",
    "    test_dataset = FeederINCLUDE(data_path=f\"wsl100_test_data_preprocess.npy\", label_path=f\"wsl100_test_label_preprocess.npy\")\n",
    "    valid_dataset = FeederINCLUDE(data_path=f\"wsl100_valid_data_preprocess.npy\", label_path=f\"wsl100_valid_label_preprocess.npy\")\n",
    "    '''\n",
    "    %cd /home/ibmelab/Documents/GG/VSLRecognition/AUTSL/AAGCN/\n",
    "    train_dataset = FeederINCLUDE(data_path=f\"autsl_train_data_preprocess.npy\", label_path=f\"train_label_preprocess.npy\",\n",
    "                            transform=transforms)\n",
    "    test_dataset = FeederINCLUDE(data_path=f\"autsl_test_data_preprocess.npy\", label_path=f\"test_label_preprocess.npy\")\n",
    "    valid_dataset = FeederINCLUDE(data_path=f\"autsl_valid_data_preprocess.npy\", label_path=f\"valid_label_preprocess.npy\")\n",
    "\n",
    "    # DataLoader\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "    val_dataloader = DataLoader(valid_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    specific_batch = next(iter(train_dataloader))\n",
    "    print(\"Input shape \", specific_batch[0].shape)\n",
    "    print(\"Data loader success\")\n",
    "    # Trainer PL\n",
    "    %cd /home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN\n",
    "    trainer = pl.Trainer(max_epochs = 120, accelerator=\"auto\", check_val_every_n_epoch = 1, \n",
    "                       devices = 1, callbacks=callbacks)\n",
    "                    #  , logger=wandb_logger) # wandb\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "    # Test PL (When test find the right ckpt_path and comment code line 58)\n",
    "    # trainer.test(model, test_dataloader, ckpt_path=\"checkpoints/epoch=61-valid_accuracy=0.91-vsl_100-aagcn-2hand+preprocessing_keypoint+augment(v1).ckpt\", \n",
    "                # verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the NumPy file\n",
    "# array = np.load('/home/ibmelab/Documents/GG/VSLRecognition/vsl/AAGCN/vsl199_train_data_preprocess.npy')\n",
    "# array = np.load('/home/ibmelab/Documents/GG/VSLRecognition/vsl/gcn_keypoints_v2/01_Co-Hien_1-100_1-2-3_0108___center_device02_signer01_center_ord1_2/hand_flow_00000.npy')\n",
    "array = np.load('/home/ibmelab/Documents/GG/VSLRecognition/vsl/poseflow/01_Co-Hien_1-100_1-2-3_0108___center_device02_signer01_center_ord1_2/flow_00007.npy')\n",
    "# Print the shape of the array\n",
    "print(array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN\n",
      "[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9), (10, 10), (11, 11), (12, 12), (13, 13), (14, 14), (15, 15), (16, 16), (17, 17), (18, 18), (19, 19), (20, 20), (21, 21), (22, 22), (23, 23), (24, 24), (25, 25), (26, 26), (27, 27), (28, 28), (29, 29), (30, 30), (31, 31), (32, 32), (33, 33), (34, 34), (35, 35), (36, 36), (37, 37), (38, 38), (39, 39), (40, 40), (41, 41), (42, 42), (43, 43), (44, 44), (45, 45), (41, 37), (37, 39), (39, 38), (38, 40), (41, 22), (22, 23), (23, 21), (21, 24), (22, 26), (26, 27), (27, 25), (25, 28), (26, 34), (34, 35), (35, 33), (33, 36), (41, 30), (30, 31), (31, 29), (29, 32), (20, 16), (16, 18), (18, 17), (17, 19), (20, 1), (1, 2), (2, 0), (0, 3), (1, 5), (5, 6), (6, 4), (4, 7), (5, 13), (13, 14), (14, 12), (12, 15), (20, 9), (9, 10), (10, 8), (8, 11), (42, 45), (45, 20), (42, 43), (43, 44), (44, 41)]\n",
      "/home/ibmelab/Documents/GG/VSLRecognition/vsl/AAGCN\n",
      "/home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "2024-10-10 16:04:00.538382: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-10 16:04:00.556358: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-10 16:04:00.561740: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-10 16:04:00.575939: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-10 16:04:01.355515: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/ibmelab/anaconda3/envs/son/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\n",
      "   | Name    | Type               | Params\n",
      "------------------------------------------------\n",
      "0  | data_bn | BatchNorm1d        | 184   \n",
      "1  | l1      | TCN_GCN_unit       | 52.4 K\n",
      "2  | l2      | TCN_GCN_unit       | 69.9 K\n",
      "3  | l3      | TCN_GCN_unit       | 69.9 K\n",
      "4  | l4      | TCN_GCN_unit       | 69.9 K\n",
      "5  | l5      | TCN_GCN_unit       | 232 K \n",
      "6  | l6      | TCN_GCN_unit       | 252 K \n",
      "7  | l7      | TCN_GCN_unit       | 252 K \n",
      "8  | fc      | Linear             | 25.7 K\n",
      "9  | loss    | CrossEntropyLoss   | 0     \n",
      "10 | metric  | MulticlassAccuracy | 0     \n",
      "------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.100     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc063a0e4174236b3a77e44771d6572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ibmelab/anaconda3/envs/son/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/home/ibmelab/anaconda3/envs/son/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (31) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48514482234643ffa7635299144184e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6152a5b4ad4498f9fc13236509cb81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503d55dea23f40b991f5c30653faec72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae94c7e7994d43c89e8938c6205ab7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa49871283f4bdbb29f0d83b31f43e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb70dc93b144e068b0e8a99a62cdc19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd1b20c3e67496499e1d83d33a45ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0407b2fa2646b8984f15ad370fc242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe57b3180dc485296cbee43fad696c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78bd470d58ee42eca5aca3415e3e71b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4508c226247043ae9ddeda5464ae33d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed37b5b8426d46ee891862225dc9bea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e85cff495a4b528ba184d8aac80b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6751a72a98f4e8b875a294572cd668c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517e4c25c3874d048545e8b394e7cb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e262f68327bf43fb8bb6dc7feb802142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536954514f0e4ce7a4e52584941e1201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cdcda75a14e4b248e5b5007174c8bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a32c4ea1741436b8e1d6e6ca890b99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0324f31c13ee4c61b28166b3e59ff76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37340f8d1a3341d6acf57974daff6d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25bb64e7c747436bbb2f404933aecb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5de7781432c4471857720de39d7480d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97003274ac14f249142b628b3d37433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b970fd30214ba0aa9b6cdf0bf944be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312e38a333ac489791fb77c9de4b583a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4033dc9d5bc4da091e3625b86cfc2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40f76ad9af44f82b05f4aa27093295e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a970d6c25fc4ab6b698d946a0da20d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf6f445d9ee4fd09606ea4eb280d319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c31484ca014bdaaef7b6acd84142c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb343e8f768243a28abe99dbc1a46862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb8c408c836416e945fbd77b7a2d582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934a0559fcdb4e1aa82422fb5bde1c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dcf1c1c9e7e4fc59d8d93cadad5785f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002275d8b2874080b273503252a36d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8350fce0b5934cad88a9d5c67b86124e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554341cb03db45adacf4262ad6873f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d5f016304e453b9d3258aafac532d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "257758c1fa6a47d2935c98eeedc82e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2aee4c3c3c41c8b429a876b104163e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120605b7958a460b82cbbe74ba72dd88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d9b8699d9e45adaf00c655cec94596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9adcf72373484d289a089372f6bd9cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d224d0f9cd4213b213eaf634db3b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6572b3b379ce43c6893f0a0e6382cd96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beef7e821e554fbbac2e80eabdcf9a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30888951239460a9a1bf2118df61ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3e7cff7ff8495e98adb7ee01ead1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa7f4a6eade4f8dbe24d5c6e23bc136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0341cc2faf0d423b83a3d94aaddbbe06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c4b843fb654d5ca2cc73227464b04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c246fc3ffe47f89a4fa3823db5e3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9531059c1b004b018881159555727c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74b3a2321c54b6da84a23934f1a8e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faff167397ce456bbdf1f27bfada3558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa8a294f6a24e8f929f5b62c030037d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c2c26ccacb48d89df1b2e31e352338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efb91d36e9b411895ec22496425262c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35e69c677fd45278e5a217fd2db6eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04aea73ead304ef0ba49f4271529c704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac992c5a5e7b4354b03214bb97e77f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8655bc7e4c3a4d2bb03bbe9d3980f1ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffaca9bc564f450bb306cad77b4be5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beec8c0ed4d94bff8ffc253d062fceae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db38fc7056a24bdc8ce19e555a69da20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49569a0419e4e24aa7d6096aab02901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce5fcd1ef9b44bb94ccf6995a63ad40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9adce3d6c584259b21946536839eefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8ac4232b264e49b654c134803231dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59edb589c7dd453e82098f32d295aa9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc9b021881e428f8b12a3e48a5f9193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e88d8d8fe87433d870865f2c6e19cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268094a5c7524b5fa8f97f867201ee8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7349392642c45a187df51559a016fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e216dce7a384595a63939be4a9cc6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8afa98bebe2f4af993603d5135541d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2952a9b5b0f4c0a9b3ca0fe4a3d074d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949f13aa22bd491586dd6d20215763d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e030f663cc984a60bbb649a8d9dfb333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55325e29802d465bbba720d190d51777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbea8a708e74399b368bb7912cfef3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07c3751899546b5a8860fef62c8c96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd627d6c88ba4a7f9c8c0e8175a9d6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce95df8386747f390a456dab9682821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab82b84ae9a74fdeb518d9ca12b3c738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50ea0f04fd24729acf7215cb974e05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d9c729e77647abb923c1e4ca81fde6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90304c19a364a88820433c1d18294ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9daccbda1464478597b6bf85203ca9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ef6b404e9c4870a6c3a59a057f466e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b99dd983eb4532ba30a5769b21453f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "479489414c444ab2b2c6be1a909786de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c6a243832b4202ab2eb3873d1be7a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218b9c009c8d4baf93f20496cd97a1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636b8f2ad7f440e9adbb1aba6f27d1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88049116f0584d01a529db8e7791c468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64f1d48e7544e3b95a80dbf5916a865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ef97640ae3474b9d736453bb7d26a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10de4e6d006049eeb3a1e5db0bb3a9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd13d014cd8451590cf48c2a383b03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517a8314adba45bb98ddc6303d706e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36be9c86a72a48e2a3926c04949fe587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68704baf614e49f8a18ca144166f5cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5f2e89d9484790829f9454fc916c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c76122cb214a1795f2ce50c22e29a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2adf57e4a734e90a6d6d687ce95d651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8177e453b644eda301e5929709cd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f961e561944c748a5e6dbc2dbd576f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271c2c72a949414fb842e7d1ab1dbc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e964bbe5dd546f684becbfd22b37141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98782ccd36fa46a495fdadb760b22f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6f1213c7734ebd94f86680c415221c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e198c7c6354319b8882bb192026390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e49c0a2fa7d046d0bbd565147612ed4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165fe5f4310f425bac1636f582726dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9d460b82e84c4e89d452f410be94b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a4bfdf8f8b477d862697d861e51ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da1dfd11f9c43eab4e39385b8054d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76b770bbcd04922893bba04080e4ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c75e1e12c5418b863f999b4e131700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=120` reached.\n",
      "/home/ibmelab/anaconda3/envs/son/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN/\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from feeder import FeederINCLUDE, FeederCustomV2\n",
    "from aagcn import AAGCN\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from augumentation import Rotate, Compose, GaussianNoise\n",
    "from pytorch_lightning.utilities.migration import pl_legacy_patch\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    config = {'batch_size': 140, 'learning_rate': 0.0137296, 'weight_decay': 0.000150403}\n",
    "    num_labels = 199  # Set your new number of classes here\n",
    "\n",
    "    # Load device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Initialize the model with the new number of classes\n",
    "    model = AAGCN(\n",
    "        num_class=num_labels,\n",
    "        num_point=46,\n",
    "        num_person=1,\n",
    "        in_channels=2,\n",
    "        graph_args={\"layout\": \"mediapipe_two_hand\", \"strategy\": \"spatial\"},\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    # Path to your checkpoint\n",
    "    checkpoint_path = \"checkpoints/epoch=37-valid_accuracy=0.88-autsl-aagcn-smaller-model.ckpt\"\n",
    "\n",
    "    # Load the checkpoint\n",
    "    with pl_legacy_patch():\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # Get the state dict\n",
    "    state_dict = checkpoint['state_dict']\n",
    "\n",
    "    # Remove the keys for the final layer (adjust 'fc' to match your model's final layer name)\n",
    "    filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith('fc.')}\n",
    "\n",
    "    # Load the filtered state dict into the model\n",
    "    model.load_state_dict(filtered_state_dict, strict=False)\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            dirpath=\"checkpoints\",\n",
    "            monitor=\"valid_loss\",\n",
    "            mode=\"min\",\n",
    "            every_n_epochs=2,\n",
    "            filename='{epoch}-{valid_accuracy:.2f}-vsl199-small-model'\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    transforms = Compose([\n",
    "        Rotate(15, 80, 25, (0.5, 0.5)),\n",
    "        GaussianNoise(),\n",
    "    ])\n",
    "    \n",
    "\n",
    "    %cd /home/ibmelab/Documents/GG/VSLRecognition/vsl/AAGCN/\n",
    "    # Datasets\n",
    "    train_dataset = FeederINCLUDE(\n",
    "        data_path=\"vsl199_train_data_preprocess.npy\",\n",
    "        label_path=\"train_label_preprocess.npy\",\n",
    "        transform=transforms\n",
    "    )\n",
    "    test_dataset = FeederINCLUDE(\n",
    "        data_path=\"vsl199_test_data_preprocess.npy\",\n",
    "        label_path=\"test_label_preprocess.npy\"\n",
    "    )\n",
    "    valid_dataset = FeederINCLUDE(\n",
    "        data_path=\"vsl199_val_data_preprocess.npy\",\n",
    "        label_path=\"val_label_preprocess.npy\"\n",
    "    )\n",
    "    # def gcn_bert_collate_fn_(batch):\n",
    "    #     labels = torch.stack([s[1] for s in batch],dim = 0)\n",
    "    #     keypoints = torch.stack([s[0] for s in batch],dim = 0) # bs t n c\n",
    "                                                                                                             \n",
    "    #     return {'keypoints':keypoints},labels\n",
    "\n",
    "    # collate_func = gcn_bert_collate_fn_\n",
    "    # train_dataset = FeederCustomV2('/home/ibmelab/Documents/GG/VSLRecognition/vsl','train')\n",
    "    # test_dataset = FeederCustomV2('/home/ibmelab/Documents/GG/VSLRecognition/vsl','test')\n",
    "    # valid_dataset = FeederCustomV2('/home/ibmelab/Documents/GG/VSLRecognition/vsl','val')\n",
    "\n",
    "    # DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True,\n",
    "                                  num_workers = 12, prefetch_factor = 4, persistent_workers =  True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False,\n",
    "                                 num_workers = 12, prefetch_factor = 4, persistent_workers =  True)\n",
    "    val_dataloader = DataLoader(valid_dataset, batch_size=config[\"batch_size\"], shuffle=False,\n",
    "                                num_workers = 12, prefetch_factor = 4, persistent_workers =  True)\n",
    "\n",
    "    %cd /home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN/\n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=120,\n",
    "        accelerator=\"auto\",\n",
    "        check_val_every_n_epoch=1,\n",
    "        devices=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN/\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from feeder import FeederINCLUDE, FeederCustomV2\n",
    "from aagcn import AAGCN\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from augumentation import Rotate, Compose\n",
    "from pytorch_lightning.utilities.migration import pl_legacy_patch\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    config = {'batch_size': 90, 'learning_rate': 0.0137296, 'weight_decay': 0.000150403}\n",
    "    num_labels = 199  # Set your new number of classes here\n",
    "\n",
    "    # Load device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Initialize the model with the new number of classes\n",
    "    model = AAGCN(\n",
    "        num_class=num_labels,\n",
    "        num_point=46,\n",
    "        num_person=1,\n",
    "        in_channels=2,\n",
    "        graph_args={\"layout\": \"mediapipe_two_hand\", \"strategy\": \"spatial\"},\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    # Path to your checkpoint\n",
    "    checkpoint_path = \"checkpoints/epoch=95-valid_accuracy=0.73-vsl199.ckpt\"\n",
    "\n",
    "    # Load the checkpoint\n",
    "    with pl_legacy_patch():\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # Get the state dict\n",
    "    state_dict = checkpoint['state_dict']\n",
    "\n",
    "    # Remove the keys for the final layer (adjust 'fc' to match your model's final layer name)\n",
    "    # filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith('fc.')}\n",
    "\n",
    "    # Load the filtered state dict into the model\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            dirpath=\"checkpoints\",\n",
    "            monitor=\"valid_loss\",\n",
    "            mode=\"min\",\n",
    "            every_n_epochs=2,\n",
    "            filename='{epoch}-{valid_accuracy:.2f}-vsl199-FeederCustom'\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    transforms = Compose([\n",
    "        Rotate(15, 80, 25, (0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "\n",
    "    %cd /home/ibmelab/Documents/GG/VSLRecognition/vsl/AAGCN/\n",
    "    # Datasets\n",
    "    train_dataset = FeederINCLUDE(\n",
    "        data_path=\"vsl199_train_data_preprocess.npy\",\n",
    "        label_path=\"train_label_preprocess.npy\",\n",
    "        transform=transforms\n",
    "    )\n",
    "    test_dataset = FeederINCLUDE(\n",
    "        data_path=\"vsl199_test_data_preprocess.npy\",\n",
    "        label_path=\"test_label_preprocess.npy\"\n",
    "    )\n",
    "    valid_dataset = FeederINCLUDE(\n",
    "        data_path=\"vsl199_val_data_preprocess.npy\",\n",
    "        label_path=\"val_label_preprocess.npy\"\n",
    "    )\n",
    "    # def gcn_bert_collate_fn_(batch):\n",
    "    #     labels = torch.stack([s[1] for s in batch],dim = 0)\n",
    "    #     keypoints = torch.stack([s[0] for s in batch],dim = 0) # bs t n c\n",
    "                                                                                                             \n",
    "    #     return {'keypoints':keypoints},labels\n",
    "\n",
    "    # collate_func = gcn_bert_collate_fn_\n",
    "    # train_dataset = FeederCustomV2('/home/ibmelab/Documents/GG/VSLRecognition/vsl','train')\n",
    "    # test_dataset = FeederCustomV2('/home/ibmelab/Documents/GG/VSLRecognition/vsl','test')\n",
    "    # valid_dataset = FeederCustomV2('/home/ibmelab/Documents/GG/VSLRecognition/vsl','val')\n",
    "\n",
    "    # DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True,\n",
    "                                  num_workers = 12, prefetch_factor = 4, persistent_workers =  True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False,\n",
    "                                 num_workers = 12, prefetch_factor = 4, persistent_workers =  True)\n",
    "    val_dataloader = DataLoader(valid_dataset, batch_size=config[\"batch_size\"], shuffle=False,\n",
    "                                num_workers = 12, prefetch_factor = 4, persistent_workers =  True)\n",
    "\n",
    "    %cd /home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN/\n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=120,\n",
    "        accelerator=\"auto\",\n",
    "        check_val_every_n_epoch=1,\n",
    "        devices=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at checkpoints/epoch=15-valid_accuracy=0.70-vsl199-small-model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9), (10, 10), (11, 11), (12, 12), (13, 13), (14, 14), (15, 15), (16, 16), (17, 17), (18, 18), (19, 19), (20, 20), (21, 21), (22, 22), (23, 23), (24, 24), (25, 25), (26, 26), (27, 27), (28, 28), (29, 29), (30, 30), (31, 31), (32, 32), (33, 33), (34, 34), (35, 35), (36, 36), (37, 37), (38, 38), (39, 39), (40, 40), (41, 41), (42, 42), (43, 43), (44, 44), (45, 45), (41, 37), (37, 39), (39, 38), (38, 40), (41, 22), (22, 23), (23, 21), (21, 24), (22, 26), (26, 27), (27, 25), (25, 28), (26, 34), (34, 35), (35, 33), (33, 36), (41, 30), (30, 31), (31, 29), (29, 32), (20, 16), (16, 18), (18, 17), (17, 19), (20, 1), (1, 2), (2, 0), (0, 3), (1, 5), (5, 6), (6, 4), (4, 7), (5, 13), (13, 14), (14, 12), (12, 15), (20, 9), (9, 10), (10, 8), (8, 11), (42, 45), (45, 20), (42, 43), (43, 44), (44, 41)]\n",
      "/home/ibmelab/Documents/GG/VSLRecognition/vsl/AAGCN\n",
      "Input shape  torch.Size([170, 2, 80, 46, 1])\n",
      "Data loader success\n",
      "/home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "Loaded model weights from the checkpoint at checkpoints/epoch=15-valid_accuracy=0.70-vsl199-small-model.ckpt\n",
      "/home/ibmelab/anaconda3/envs/son/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da43e58eae854f17b609ea0045935a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets :  tensor([  1,  10, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,  11, 110,\n",
      "        111, 112, 113, 114, 115, 116, 117, 118, 119,  12, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127, 128, 129,  13, 130, 131, 132, 133, 134, 135, 136,\n",
      "        137, 138, 139,  14, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149,\n",
      "         15, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159,  16, 160, 161,\n",
      "        162, 163, 164, 165, 166, 167, 168, 169,  17, 170, 171, 172, 173, 174,\n",
      "        175, 176, 177, 178, 179,  18, 180, 181, 182, 183, 184, 185, 186, 187,\n",
      "        188, 189,  19, 190, 191, 192, 193, 194, 195, 196, 197, 198,   2,  20,\n",
      "         21,  22,  23,  24,  25,  26,  27,  28,  29,   3,  30,  31,  32,  33,\n",
      "         34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  45,  46,  47,\n",
      "         48,  49,   5,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,   6,\n",
      "         60,  61,  62,  63,  64,  65,  66,  67,  68,  69,   7,  70,  71,  72,\n",
      "         73,  74], device='cuda:0')\n",
      "Preds :  tensor([  1,  10, 100, 101, 102, 115, 104, 105, 106, 107, 108, 109,  11, 110,\n",
      "        111, 112, 114, 114, 115, 116, 117, 118, 119,  12, 120, 121, 122, 123,\n",
      "        124, 125, 126, 122, 128, 129,  13,  88, 131, 132, 133, 134, 135, 128,\n",
      "        137, 138, 139,  14, 140,  32, 142, 143, 144, 145, 146, 147, 148, 149,\n",
      "         15, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159,  16, 160, 161,\n",
      "        162, 163, 164, 165, 166, 167, 168, 169, 184, 170, 171, 172, 173, 174,\n",
      "        175, 176, 177, 178, 179, 131, 180, 181, 182, 183, 184, 185, 186, 187,\n",
      "        188, 189,  19, 190, 191, 192, 193, 194, 195, 196, 197, 198,   2, 156,\n",
      "         21,  22,  23,  24,  25,  26,  27,  28,  29,   3,  30,  31,  32,  33,\n",
      "         34,  35,  36,  37,  38,  39,  40, 150,  42,  43,  44,  45,  46,  47,\n",
      "         48,  49,   5,  50,  51,  52,  53,  54,  55,  56, 182,  58,  59,   6,\n",
      "         60,  61,  62,  63,  64,  65,  66,  67,  68,  69,   7,  70,  71,  72,\n",
      "         73, 137], device='cuda:0')\n",
      "Targets :  tensor([ 75,  76,  77,  78,  79,   8,  80,  81,  82,  83,  84,  85,  86,  87,\n",
      "         88,  89,   9,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99,   0,\n",
      "          1,  10, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,  11, 110,\n",
      "        111, 112, 113, 114, 115, 116, 117, 118, 119,  12, 121, 122, 123, 124,\n",
      "        125, 126, 127, 128, 129,  13, 130, 131, 132, 133, 134, 135, 136, 137,\n",
      "        138, 139,  14, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149,  15,\n",
      "        150, 151, 152, 153, 154, 155, 156, 157, 158, 159,  16, 160, 161, 162,\n",
      "        163, 164, 165, 166, 167, 168, 169,  17, 170, 171, 172, 173, 174, 175,\n",
      "        176, 177, 178, 179,  18, 180, 181, 182, 183, 184, 185, 186, 187, 188,\n",
      "        189,  19, 190, 191, 192, 193, 194, 195, 196, 197, 198,   2,  20,  21,\n",
      "         22,  23,  24,  25,  26,  27,  28,  29,   3,  30,  31,  32,  33,  34,\n",
      "         35,  36,  37,  38,  39,   4,  40,  41,  42,  43,  44,  45,  46,  47,\n",
      "         48,  49], device='cuda:0')\n",
      "Preds :  tensor([ 75,  76,  77,  78,  79,   8,  80,  81,  82,  83,  84,  85,  86,  87,\n",
      "         88,  89,   9,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99,   0,\n",
      "          1,  10, 100, 101, 102, 115, 163,  39, 106, 107, 108, 109,  11, 110,\n",
      "        111, 112, 113, 113, 115, 189, 174,  63,  40,  12, 121, 122, 123, 124,\n",
      "        194, 126, 127, 128, 129, 164, 130, 131, 132, 133, 134, 135, 182, 137,\n",
      "        188, 139,  14, 135, 141, 142, 143, 144, 145, 146, 147, 115, 149,  15,\n",
      "         41, 151, 152, 153, 154, 155, 162, 110, 158, 152,  66, 160, 161, 162,\n",
      "        163, 135, 165,  30, 167, 189, 163,  53, 170, 173,  58, 173, 174, 175,\n",
      "        176, 177, 116, 179,  18, 180, 181,  47, 183, 184, 185, 186, 187, 189,\n",
      "        189,  19, 186,  86, 192,  88, 194, 195, 196, 127, 198,   2,  20, 197,\n",
      "         22,  23,  24,  25,  26,  27,  28,  29,   3,  30,  31, 102,  33,  34,\n",
      "         35,  36,  37, 108,  39,   4,  40,  41,  42,  43,  44,  45,  46,  47,\n",
      "         48,  49], device='cuda:0')\n",
      "Targets :  tensor([  5,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,   6,  60,  61,\n",
      "         62,  63,  64,  65,  66,  67,  68,  69,   7,  70,  71,  72,  73,  74,\n",
      "         75,  76,  77,  78,  79,   8,  80,  81,  82,  83,  84,  85,  86,  87,\n",
      "         88,  89,   9,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99,   0,\n",
      "          1,  10, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,  11, 110,\n",
      "        111, 112, 113, 114, 115, 116, 117, 118, 119,  12, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127, 128, 129,  13, 130, 131, 132, 133, 134, 135, 136,\n",
      "        137, 138, 139,  14, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149,\n",
      "         15, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159,  16, 160, 161,\n",
      "        162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175,\n",
      "        176, 177, 178, 179,  18, 180, 181, 182, 183, 184, 185, 186, 187, 188,\n",
      "        189,  19, 190, 191, 192, 193, 194, 195, 196, 197, 198,   2,  20,  21,\n",
      "         22,  23], device='cuda:0')\n",
      "Preds :  tensor([  5,  50,  51,  29,  53, 161,  55, 144,  57,  58,  59, 103,  60,  61,\n",
      "         62,  63,  64,  65,  79,  25,  88,  69,  22,  70,  71,  72,  73,  74,\n",
      "         75,  76,  77,  78,  79,   8,  80,  81,  38,  83,  84,  85,  86,  87,\n",
      "         88,  89,   9, 107,  36,  92,  93,  94,  95,  97,  97,  98,  99,   0,\n",
      "          1,  10, 100, 101, 102, 103, 107, 105, 106, 107, 108, 109,  11, 110,\n",
      "        111, 112, 114, 114, 115, 116, 117, 118, 119,  12, 120, 121, 122, 123,\n",
      "        124, 125, 126, 122, 128, 130,  13, 130, 131, 132, 133, 134, 135, 136,\n",
      "        137, 138, 139,  14, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149,\n",
      "         15, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159,  16, 160, 161,\n",
      "        162, 163, 164, 165, 166, 185,  48, 169, 170, 171, 172, 173, 174, 107,\n",
      "        176, 177, 175, 179,  18, 180, 181, 157, 183,  53, 185, 186, 187,  35,\n",
      "        189,  19, 190, 191, 192, 193, 194, 195, 196, 197, 198,   2,  20,  21,\n",
      "         22,  23], device='cuda:0')\n",
      "Targets :  tensor([ 24,  25,  26,  27,  28,  29,   3,  30,  31,  32,  33,  34,  35,  36,\n",
      "         37,  38,  39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,   5,\n",
      "         50,  51,  52,  53,  54,  55,  56,  57,  58,  59,   6,  60,  61,  62,\n",
      "         63,  64,  65,  66,  67,  68,  69,   7,  70,  71,  72,  73,  74,  75,\n",
      "         76,  77,  78,  79,   8,  80,  81,  82,  83,  84,  85,  86,  87,  88,\n",
      "         89,   9,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99,   0,   1,\n",
      "         10, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,  11, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119,  12, 120, 121, 122, 123, 124,\n",
      "        125, 126, 127, 128, 129,  13, 130, 131, 132, 133, 134, 135, 136, 137,\n",
      "        138, 139,  14, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149,  15,\n",
      "        150, 151, 152, 153, 154, 155, 156, 157, 158, 159,  16, 160, 161, 162,\n",
      "        163, 164, 165, 166, 167, 168, 169,  17, 170, 171, 172, 173, 174, 175,\n",
      "        176, 177], device='cuda:0')\n",
      "Preds :  tensor([ 24,  25,  26,  27,  28,  29,   3, 191,  31,  32,  33,  34,  35, 179,\n",
      "         37,  38,  39,  40,  41,  42,  24,  44,  45,  46,  47,  48,  49,   5,\n",
      "         50,  51,  10,  53,  54,  55,  56,  57,  58,  59,   6,  60,  47,  62,\n",
      "         63,  64,  65,  66,  67,  68,  69,   7,  70,  36,  72,  73, 137,  75,\n",
      "         76,   2,  78,  79,   8,  80,  81,  82,  83,  84,  85,  86,  87,  88,\n",
      "         89,   9,  90, 117,  92,  93,  94,  95, 197,  97,  98,  99, 132,   1,\n",
      "         10, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,  11, 110, 111,\n",
      "        112, 182, 136,   8,  90, 117, 118, 119,  12, 120, 121, 122, 123, 124,\n",
      "        125, 126, 127, 128, 129,  13, 130, 131, 132, 133, 146, 135, 136, 137,\n",
      "         35, 139,  14, 140, 141,  49, 143, 144, 145, 146, 147, 148, 149,  15,\n",
      "        150, 151, 152, 153, 154, 155, 156, 157, 158, 159,  16, 160,  31, 162,\n",
      "        163,  13, 165, 166, 167, 168, 169,  17,  98, 171, 172, 173, 174, 138,\n",
      "         68, 177], device='cuda:0')\n",
      "Targets :  tensor([178, 179,  18, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189,  19,\n",
      "        190, 191, 192, 193, 194, 195, 196, 197, 198,   2,  20,  21,  22,  23,\n",
      "         24,  25,  26,  27,  28,  29,   3,  30,  31,  32,  33,  34,  35,  36,\n",
      "         37,  38,  39,   4,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,\n",
      "          5,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,   6,  60,  61,\n",
      "         62,  63,  64,  65,  66,  67,  68,  69,   7,  70,  71,  72,  73,  74,\n",
      "         75,  76,  77,  78,  79,   8,  80,  81,  82,  83,  84,  85,  86,  87,\n",
      "         88,  89,   9,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99],\n",
      "       device='cuda:0')\n",
      "Preds :  tensor([178, 179,  14, 180, 181, 182, 183, 184, 185, 186,  97, 188, 189,  19,\n",
      "         94, 191, 192, 193, 194, 195, 196, 197, 198,   2,  20,  21,  22,  23,\n",
      "         24, 110,  26,  27,  28,  29,   3,  30,  31, 141,  33, 149,  35,  36,\n",
      "         37,  38,  39,   4,  40,  41,  42,  43,  17,  45, 105,  47,  48,  49,\n",
      "          0,  50,  51,  52,  53,  54,  55, 183,  57,  58,  59,   6,  60,  61,\n",
      "         62,  63,  64,  65,  66,  67,  68,  69,   7,  70,  71,  72,  73,  74,\n",
      "          9,  76,  77, 132,  79,   8,  80,  81,  38, 182,  84,  85,  86,  87,\n",
      "         88,  89,   9,  90,  91,  92,  93,  94, 189,  96,  97,  98, 161],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.835220217704773     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5644031167030334     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.835220217704773    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5644031167030334    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%cd /home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "from feeder import FeederINCLUDE\n",
    "from aagcn import AAGCN\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import wandb\n",
    "from augumentation import Rotate, Left, Right, GaussianNoise, Compose\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Hyper parameter tuning : batch_size, learning_rate, weight_decay\n",
    "    #batch_size: 2->1\n",
    "    config = {'batch_size': 170, 'learning_rate': 0.0137296, 'weight_decay': 0.000150403}\n",
    "    # Load device\n",
    "    device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # Load model\n",
    "\n",
    "    # num_class: 101 -> 3 (= number of labels)\n",
    "    model = AAGCN(num_class=199, num_point=46, num_person=1, in_channels=2,\n",
    "                graph_args = {\"layout\" :\"mediapipe_two_hand\", \"strategy\": \"spatial\"},\n",
    "                learning_rate=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "    # Callback PL\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            dirpath=\"checkpoints\",\n",
    "            monitor=\"valid_accuracy\",\n",
    "            mode=\"max\",\n",
    "            every_n_epochs = 2,\n",
    "            filename='{epoch}-{valid_accuracy:.2f}-wsl_100-aagcn-{fold}'\n",
    "        ),\n",
    "    ]\n",
    "    # Augument \n",
    "    batch_size = config[\"batch_size\"]\n",
    "    transforms = Compose([\n",
    "        Rotate(15, 80, 25, (0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # Dataset class\n",
    "    ''' Đổi tên path\n",
    "    train_dataset = FeederINCLUDE(data_path=f\"wsl100_train_data_preprocess.npy\", label_path=f\"wsl100_train_label_preprocess.npy\",\n",
    "                            transform=transforms)\n",
    "    test_dataset = FeederINCLUDE(data_path=f\"wsl100_test_data_preprocess.npy\", label_path=f\"wsl100_test_label_preprocess.npy\")\n",
    "    valid_dataset = FeederINCLUDE(data_path=f\"wsl100_valid_data_preprocess.npy\", label_path=f\"wsl100_valid_label_preprocess.npy\")\n",
    "    '''\n",
    "    %cd /home/ibmelab/Documents/GG/VSLRecognition/vsl/AAGCN/\n",
    "    train_dataset = FeederINCLUDE(data_path=f\"vsl199_train_data_preprocess.npy\", label_path=f\"train_label_preprocess.npy\",\n",
    "                            transform=transforms)\n",
    "    test_dataset = FeederINCLUDE(data_path=f\"vsl199_test_data_preprocess.npy\", label_path=f\"test_label_preprocess.npy\")\n",
    "    valid_dataset = FeederINCLUDE(data_path=f\"vsl199_val_data_preprocess.npy\", label_path=f\"val_label_preprocess.npy\")\n",
    "\n",
    "    # DataLoader\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "    val_dataloader = DataLoader(valid_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    specific_batch = next(iter(train_dataloader))\n",
    "    print(\"Input shape \", specific_batch[0].shape)\n",
    "    print(\"Data loader success\")\n",
    "    # Trainer PL\n",
    "    %cd /home/ibmelab/Documents/GG/VSLRecognition/HandSignRecogDev/AAGCN\n",
    "    trainer = pl.Trainer(max_epochs = 120, accelerator=\"auto\", check_val_every_n_epoch = 1, \n",
    "                       devices = 1, callbacks=callbacks)\n",
    "                    #  , logger=wandb_logger) # wandb\n",
    "    #trainer.fit(model, train_dataloader, val_dataloader)\n",
    "    # Test PL (When test find the right ckpt_path and comment code line 58)\n",
    "    trainer.test(model, test_dataloader, ckpt_path=\"checkpoints/epoch=15-valid_accuracy=0.70-vsl199-small-model.ckpt\", \n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "son",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
